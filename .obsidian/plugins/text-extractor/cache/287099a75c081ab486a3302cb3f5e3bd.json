{"path":"Files/Ling 430 - Handout #7 - BoW.pdf","text":"Ling 430 February 4 th, 2022 Bag of Words 1. Document Semantics, cont’d • Where are we now in the pipeline of document semantics? i) Tokenization ii) Normalization - case folding - stemming and lemmatization ` iii) Vectorization • Computers “think” in numbers. We have to turn our words into numbers, so we can “calculate” the meaning. • Vectorization is the process of turning words and documents into numbers. A vector is a one-dimensional set of values. 2. One-hot encoding • One-hot encoding is a means of converting data (in this case, words) into binary vectors--- 0 (off) and 1 (on / hot) are the only values • Each word in our sentence or document will be a vector, where each element in the vector indicates the identity of the word. • A sentence then will be a collection of vectors, a 2d array, representing the number of words in the sentence. Let’s one-hot encode this sentence: “Everyone loves computational linguistics.” We can make a mini-dictionary that represents all the words we’ll encounter in this sentence: {“everyone”, “loves” , “computational” , “linguistics”} ; each of these words is represented by one element in the vector, or column within the array. The sentence is four words long, so we have four word vectors to produce. The first will be: ‘everyone’ ‘loves’ ‘computational’ ‘linguistics’ word1 1 0 0 0 ‘word1’ has the value “1” for the “everyone” column, “0” for everything else. The entire sentence vector then is: ‘everyone’ ‘loves’ ‘computational’ ‘linguistics’ word1 1 0 0 0 word2 0 1 0 0 word3 0 0 1 0 word4 0 0 0 1 Exercise: Let’s program one-hot encoding of this basic sentence. 0) Tokenize the sentence, using split(“ “), but don’t worry about stemming or lemmatization for this simple example. 1) Make a set that collects all the unique words in the sentence Alphabetize the set with the sorted() function Convert the set back to a list, and call it vocab 2) Make an empty numpy array full of zeros with the number of columns equal to the number of unique word tokens we have, and the rows equal to the number of words in the sentence: one_hot_sentence = np.zeros((len_of_sentence, number_of_unique_words), int) 3) Let’s do this step together: Loop through each word in the sentence and change the correct row and column from 0 to 1: for i, word in enumerate(tokenized_sentence): one_hot_sentence[i, vocab.index(word)] = 1 #enumerate produces (number, word) sets where the number is the position of the word in the sentence; e.g., the first iteration we’ll get (0, “everyone”) and then the variable i will be assigned as 0 and word will be assigned as “everyone”-- - then (1, “loves”), and so on #i then will be the row, and vocab.index(word) will be the column. Vocab is our set of unique words, indexed--- we can thus retrieve the index to use as the column index 4) Convert the numpy array to a pandas dataframe (not necessary, but easier to read): one_hot_df = pd.DataFrame(one_hot_sentence, columns = vocab) Question: This type of encoding retains all the features of the sentence, every word and the order it occurs in. How large will our array be if we have a vocab of 50,000 words for a book containing 100,000 words? Do you see a problem with this approach? 3. Bag of Words Vectors • Bag of words vectors encodes data about a text more economically by tossing out some information: the order of the words • Essentially, BoW vectors are just counts of the words that occur in each document i) Binary BoW: 1 or 0 for whether the word occurred in a document ii) Term frequency BoW with word counts: Tallies up the number of each word in the document = the term frequency • Now use a counter tool such as nltk’s FreqDist or the Counter tool that will both compile our vocab set and tally up the words in a text: from collections import Counter bow = Counter(text) #create Counter object You can view the n most common words in the document with the .most_common(n) method: bow.most_common(5) Exercise: Copy a couple paragraphs from a Wikipedia article of your choice. Tokenize it and feed it in a Counter object. Look at the 10-20 most common words or so. Do these give you a good idea what the article is about? Are there some words in this set that don’t seem very informative? 4. Stop Words • Stop words are common, often functional words, that are in most documents and tell us little about the overall meaning, e.g.: • We can read in a list of stop words and exclude them from our word list: import nltk stop_words = nltk.corpus.stopwords.words(‘english’) word_list = [word for word in word_list if word not in stop_words] … then feed this stop word-free list into your Counter object 5. Normalizing Word Frequency • One goal is to compare the semantic similarity between two texts Let’s count how many times ‘bird’ shows up in two texts: A Tale of Two Cities : 8 Wikipedia article about the “Forest Robin” : 3 So A Tale of Two Cities has more to do with birds, right? • Rather, let’s normalize the count by dividing by the total number of words in the text: A Tale of Two Cities : 8 / 135420 = 0.000059 Wikipedia article about the “Forest Robin” : 3 / 509 = 0.059 6. Calculating Vector Similarity • We can now represent a document as a vector. We can look at the vectors of multiple documents and calculate their similarity doc1 = “Everyone loves computational linguistics” doc2 = “Computational linguistics is a branch of linguistics” doc3 = “Dogs love to eat.” • We will want to tokenize, possibly stem, and possibly remove stop words for these documents. Then, create a lexicon that collects the set of words that occur in this collection of documents: lexicon = {“everyone”, “love”, “comput”, “linguistic”, “branch”, “dog”, “eat”} • Create a term-frequency vector for each of the three documents. The vectors for each document need to the same length and have the terms in the same order: (ev.) (lov.) (cmp.) (lng.) (brn) (dog) (eat) doc1 = [0.25 0.25 0.25 0.25 0 0 0] doc2 = [0 0 0.14 0.28 0.14 0 0] doc3 = [0 0.25 0 0 0 0.25 0.25] • We can then calculate how “close” the documents are within the vector space. Each of word in our lexicon is one dimension. We can visualize the similarity of some documents with respect to two dimensions below (but note our vectors have seven dimensions): • Now we can calculate the similarity between each pair of documents to see which ones are the closest in meaning. We will calculate the cosine similarity--- the cosine of the angle (θ, above) between the two vectors: cos 𝜃 = 𝐴 ∙ 𝐵 |𝐴| ∙ |𝐵| A and B are the two vectors; so we are taking the dot product of the two vectors here. Multiply each corresponding element together, then sum all of those together: doc1 vs. doc2: (0.25 * 0) + (0.25 * 0) + (0.25 * 0.14) + (0.25 * 0.28) + (0 * 0.14) + (0 * 0) + (0 * 0) = 0.105 We then divide this by the square root of the sum of the squares of each vector multiplied together: |A| =√ ((0.25 2) + (0.25 2) + (0.25 2) + (0.25 2) + (0 2) + (0 2) + (0 2)) |B| = √ ((02) + (0 2) + (0.142) + (0.28 2) + (0.14 2) + (02) + (0 2)) 0.105 / (0.5 * 0.342) = 0.612 #These calculations will always yield a value between -1 and 1, but since our word counts can be negative, the result should always be between 0 and 1 For doc1 vs. doc3 we get: 0.289 And then doc2 vs. doc3: 0 So 1 & 2 are most similar, followed by 1 & 3, and then 2 & 3, which should match our intuitions. Exercise: We’ll work on this together, though you should be able to code each of the steps now. 1. Read in a set of documents 2. Tokenize the documents 3. Normalize the documents--- case fold, stem or lemmatize, and remove stop words, though is optional and you may experiment with including these depending on how well the search works 4. Produce a lexicon covering all the documents 5. Vectorization: For each document, create a vector of term frequencies for each word. 6. Search and compare: Enter a document that is basically a search query (e.g., a short sentence. Calculate the cosine similarity between your query and each document. Then return the top n most similar documents based on the ones with the highest cosine similarity. or alternatively, steps 2-5 can be achieved all at once using the CountVectorizer tool, which can be applied to a list of untokenized documents: from sklearn.feature_extraction.text import CountVectorizer word_vecs = CountVectorizer() X = word_vecs.fit_transform(list_of_docs) import pandas as pd #convert the output to a dataframe, with columns labeled df = pd.DataFrame(X.todense(), columns = word_vecs.get_feature_names_out()) CountVectorizer() has several useful optional parameters: lowercase = : Automatically converts words to lowercase, but you can set to False if you prefer tokenizer = : Has a default tokenizer, but you can provide your own such as nltk’s casual_tokenize if you prefer stopwords = : Provide a list of stop words to remove from the texts 7. N-grams • Our current model for calculating similarity doesn’t take into account syntax, or even nearness of particular words--- “computational” and “linguistics” showing up somewhere in an article will be treated the same as “computational linguistics”. • An n-gram is a set of n words. A “2-gram” is also called a bigram, a “3-gram” a trigram • We can follow all the steps we did previously, but instead of counting words we can count n-grams Basic “bigramifier” code: tokenized_sent = ['everyone', 'loves', 'computational', 'linguistics'] bigrams = [] for i in range(len(tokenized_sent)-1): bigrams.append((tokenized_sent[i], tokenized_sent[i+1])) Or use the nltk tool: from nltk.util import ngrams tokenized_sent = ['everyone', 'loves', 'computational', 'linguistics'] bigrams = list(ngrams(tokenized_sent, 2)) # “2” for bigrams here What do you think are some of the drawbacks and advantages of using bigrams instead of words? When do you think using bigrams would be most appropriate?","libVersion":"0.5.0","langs":""}