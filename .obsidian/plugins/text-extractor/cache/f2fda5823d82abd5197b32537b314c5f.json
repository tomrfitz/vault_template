{"path":"Files/Ling 430 - Handout #19 - Seq2Seq.pdf","text":"Ling 430 April 11th, 2022 Seq2Seq and Machine Translation 1. Sequence-to-sequence models • Consider the task of machine translation. Perhaps we want to translate: “hello” → “hallo” “good day!” → “guten Tag!” “I see a dog” → “ich sehe einen Hund” • Think of simple ways you might start to go about this task: 1) Through ‘hardcoding’ the word relations, and issues that might involve 2) Through machine learning. How can you train a computer to learn to convert from one word to another word in a neural network? Remember, you need to convert inputs and outputs to numbers. • Now consider some more difficult examples: “I have eaten the bread” → “ich habe das Brot gegessen” “The cookies look tasty” → “die Kekse sehen lecker aus” • A sequence-to-sequence model (Seq2seq) is a method of converting from one sequence to another using neural networks, where the sequences may be of different lengths or have elements in different orders • This will be especially useful in language, say for: • translating between languages • learning answers or responses to questions or statements • learning the pronunciations of words based on the spelling • other language-like tasks, such as interpreting the phenotype of a given gene sequence 2. Basic architecture • A sequence-to-sequence model uses two LSTMs: an encoder and a decoder • The encoder converts the sequence, such as a sentence, into a lower-dimensional “thought” vector--- like an abstraction of the sentence into its underlying meaning • This “thought” vector contains the final hidden layer output of the LSTM network and its memory state, which are then used as the input for the decoder • The decoder converts the “thought” vector into the second sequence (e.g., another language) During training: The network takes both the thought vector and expected translation string of words as input. It takes this initial state + a “start” token, and tries to generate the first word, then the second, updating its weights as it goes. During application / translation: We input the thought vector as the initial state, along with a “START” token; the neural network then learns the next (i.e., first) word that is mostly likely given this state and the previous word (‘start’). The word generated is then the input for the next time step, in order to predict the next word that is most likely. 3. Coding a Seq2Seq model: Chatbot • Let’s write a chatbot trained on a seq2seq model • Basic idea: a statement is “translated” into an appropriate response • To keep this (somewhat) simple, we’ll be training it to predict the next letter rather than the next word 1) Read in the two data files. The first file contains statements with corresponding responses in the second file: data1 = \"D:/human_text.txt\" data2 = \"D:/robot_text.txt\" datafile = open(data1, 'r', encoding = 'utf-8') human_text = datafile.readlines() datafile.close datafile2 = open(data2,'r', encoding = 'utf-8') robot_text = datafile2.readlines() datafile2.close() 2) Read the two files into a single list of paired statement + response pairs and shuffle it: pairs = [(item[0], item[1]) for item in zip(human_text, robot_text)] import random random.shuffle(pairs) 3) Our “vocabulary” will be the set of unique characters used in the statements (inputs) and responses (output). I’ve added ‘\\t’ and ‘\\n’ here since they will be our start and end tokens: input_vocabulary = sorted(set([char for (statement, reply) in pairs for char in statement] )) output_vocabulary = sorted(set([char for (statement, reply) in pairs for char in reply]+['\\t', '\\n'])) 4) Pull the statements and responses apart again into separate lists, but in doing so, add the ‘start’ and ‘end’ tokens to each of the responses (targets): input_texts = [] target_texts = [] start_token = '\\t' end_token = '\\n' for statement, response in pairs: target_text = start_token + response + end_token target_texts.append(target_text) input_texts.append(statement) 5) We’re going to need to convert each statement and reply to numbers, and in this case they will be one-hot encoded string of individual characters. First, we need to know the dimensions each statement will be, so we determine the length of our vocabularies and the length of the longest statement (the two dimensions needed for all the documents): inp_vocab_size = len(input_vocabulary) out_vocab_size = len(output_vocabulary) max_inp_length = max([len(input_text) for input_text in input_texts]) max_out_length = max([len(target_text) for target_text in target_texts]) 6) We then assign values to each of the unique characters in our input and output “vocabularies”. In doing so, we want to make lookup dictionaries in order to easily convert because our characters and our values: inp_token_idx = {char : i for i, char in enumerate(input_vocabulary)} target_token_idx = {char: i for i, char in enumerate(output_vocabulary)} reverse_inp_idx = {i : char for i, char in enumerate(input_vocabulary)} reverse_target_idx = {i: char for i, char in enumerate(output_vocabulary)} 7) Now we initialize a 3d matrix of zeros that represent each statement x each character in the statement x each character in the vocab. We then fill in 1s in the appropriate cells for each document. We need to do this for our encoder inputs and the decoder inputs (with start and end tags) and decoder outputs (w/o start and end tags). import numpy as np encoder_input = np.zeros((len(input_texts), max_inp_length, inp_vocab_size), dtype = 'float32') decoder_input = np.zeros((len(input_texts), max_out_length, out_vocab_size), dtype = 'float32') decoder_output = np.zeros((len(input_texts), max_out_length, out_vocab_size),dtype = 'float32') for i, (inp_text, target_text) in enumerate(zip(input_texts, target_texts)): for pos, char in enumerate(inp_text): encoder_input[i, pos, inp_token_idx[char]] = 1 for pos, char in enumerate(target_text): decoder_input[i, pos, target_token_idx[char]] = 1 if pos > 0 : #since the output has no start tag, its positions will always be one behind decoder_output[i, pos - 1, target_token_idx[char]] = 1 8) Now we begin to write the sequence-to-sequence model. First, import the layers we need and set a few hyperparameters. We’ll be using Model instead of Sequence because of the more complex structure of the seq2seq model. We will write each component of the model and put them into “Model” at the end. from keras.models import Model from keras.layers import Input, LSTM, Dense batch_size = 16 epochs = 2 num_neurons = 256 9) Writing the encoder LSTM: Each input is one vector representing one character at time. We save the final states (state_h) and memory (state_c) which we be passed in as the decoder’s initial states. encoder_inputs = Input(shape = (None, inp_vocab_size)) encoder = LSTM(num_neurons, return_state = True) encoder_outputs, state_h, state_c = encoder(encoder_inputs) encoder_states = [state_h, state_c] #final states and memory that we input into decoder LSTM 10) The more complicated decoder LSTM below. The input is our start and end tagged strings of characters for the replies, whereas the output is the prediction based on the information stored in the encoder’s memory states (initial state assigned as “encoder_states”). We also add a dense layer which outputs the best predictions for the next character. decoder_inputs = Input(shape = (None, out_vocab_size)) decoder_lstm = LSTM (num_neurons, return_sequences = True, return_state = True) decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state = encoder_states) decoder_dense = Dense(out_vocab_size, activation = 'softmax') decoder_outputs = decoder_dense(decoder_outputs) 11) Here the three parts of the model are put together in a Model object: model = Model([encoder_inputs, decoder_inputs], decoder_outputs) 12) The model is compiled with an optimizer and loss function: model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['acc']) 13) We then fit the data to the model: model.fit([encoder_input, decoder_input], decoder_output, batch_size = batch_size, epochs = epochs, validation_split=0.1) 14) And save the model structure and weights, since this takes a long time to train: model_structure = model.to_json() with open(\"new_chat_bot.json\", \"w\") as json_file: json_file.write(model_structure) model.save_weights(\"new_chat_bot.h5\") 4. Running the chatbot • We can run a separate program to read in the saved model. We’ll need to read in new inputs and predict outputs, so the sequence-to-sequence setup looks a bit different. 1) Read in modules: from keras.models import Model from keras.layers import Input, LSTM, Dense import numpy as np 2) Read in the store model file from wherever it was saved: from keras.models import model_from_json with open(\"new_chat_bot.json\", \"r\") as json_file: json_string = json_file.read() model = model_from_json(json_string) model.load_weights('new_chat_bot.h5') 3) Set up the sequence-to-sequence model for predicting a new sequence. Here we are reading in the weights and memory states from the trained model from the file we read in: num_neurons = 256 encoder_inputs = model.input[0] encoder_outputs, state_h, state_c = model.layers[2].output encoder_states = [state_h, state_c] encoder_model = Model(encoder_inputs, encoder_states) decoder_inputs = model.input[1] decoder_state_input_h = Input(shape=(num_neurons,)) decoder_state_input_c = Input(shape=(num_neurons,)) decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c] decoder_lstm = model.layers[3] decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs) decoder_states = [state_h_dec, state_c_dec] decoder_dense = model.layers[4] decoder_outputs = decoder_dense(decoder_outputs) decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states) 4) Variables and hyperparameters for the model. These were values obtained in the training. Since I’d rather not have to go through all the original data again to come up with these values for the chatbot program, I’ve just fed the original values in directly rather than retraining it. Note the dictionaries given here are partial, and I copied them from the original training. inp_len = 228 out_len = 270 max_encoder_seq_length = 1376 max_decoder_seq_length = 972 start_token = '\\t' stop_token = '\\n' input_dict = {'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '%': 5, '&': 6, \"'\": 7 … reverse_target = {0: '\\t', 1: '\\n', 2: ' ', 3: '!', 4: '\"', 5: '#', 6: '%', 7: \"'\" … 5) Decoding the sequence: given an input (characters converted to numbers from a possible input sequence), return the thought vector from the encoding layer; Make an empty np array for the response output of maximum output length. Then, keep looping until reaching the maximum response length, each time, feeding the mostly likely next response character back into the decoder. def decode_sequence(input_seq): thought = encoder_model.predict(input_seq) target_seq = np.zeros((1, 1, out_len)) stop_condition = False generated_sequence = '' while not stop_condition: output_tokens, h, c = decoder_model.predict([target_seq] + thought) generated_token_idx = np.argmax(output_tokens[0, -1, :]) generated_char = reverse_target[generated_token_idx] generated_sequence += generated_char if (generated_char == stop_token or len(generated_sequence) > max_decoder_seq_length): stop_condition = True target_seq = np.zeros((1, 1, out_len)) target_seq[0, 0, generated_token_idx] = 1. thought = [h, c] return generated_sequence 6) Here, we have a function that converts an original input sentence into a one-hot encoded vector representing its characters. This is then fed to the decode_sequence function (above): def response(input_text): input_seq = np.zeros((1, max_encoder_seq_length, inp_len), dtype = 'float32') for t, char in enumerate(input_text): input_seq[0, t, input_dict[char]] = 1. decoded_sentence = decode_sequence(input_seq) print (input_text) print ('Bot Reply:', decoded_sentence) 7) Give various inputs to the chatbot to respond to: response('hey') response('whats up?')","libVersion":"0.5.0","langs":""}