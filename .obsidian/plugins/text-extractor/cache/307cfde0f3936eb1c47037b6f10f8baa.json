{"path":"Files/Ling 430 - Handout #4 - Processing Raw Text.pdf","text":"Ling 430 January 26 th, 2022 Processing Raw Text The NLTK toolkit has many texts and corpora available, but what if you want to process a text that is not part of NLTK? 1. Reading in a file The basic syntax for opening and reading a file, such as a .txt or .csv file, is shown below: file_name = open(PATH/document_name.txt) raw_contents = file_name.read() #a string containing the entire file file_name.close() Alternatively, you can read in the document as a list of strings, where each string is one line in the document: raw_lines = file_name.readlines() #a list of strings for each line 2. Basic Processing a) Tokenization • You probably want a list of word tokens, not lines. The simplest way of doing this is using the string method .split(split_char). • Typically the character the string is split on is a space: list_of_words = raw_lines.split(‘ ’) • The reverse of splitting is joining, if you want one string from a list of words. The join character in this case is the space once again: entire_text = ‘ ’.join(list_of_words) b) Case folding • If you are characterizing the contents of a text, you probably want ‘linguistics’ and ‘Linguistics’ to be treated the same. • Case folding converts all characters to the same case. • We saw the .lower(), .upper(), .capitalize(), and .swapcase() methods before. Typically we convert all text to lower case: list_of_words = raw_lines.split(‘ ’) list_of_words = [word.lower() for word in list_of_words] • Note here that we can’t apply .lower() to list_of_words since it is a list. You may instead want to apply to the raw string first, or only each word token to lower case during the analysis (e.g., counting the tokens) c) Trimming • Trimming removes any white space characters--- not only spaces, but formatting characters like \\t, \\n, etc. • At some point in the process, word tokens should be passed through the .strip() function: list_of_words = [word.lower().strip() for word in list_of_words] d) Replacing • You may find that some characters survive trimming, especially if they are located within strings and not at word boundaries. • You can replace characters as such: text = text.replace(‘\\n’, ‘’) #replaces new line characters with nothing e) Slicing • You may want to remove certain tokens from the beginning or end of the text. You can do this by taking a slice of the list: list_of_words = list_of_words[145:842] 3. Spreadsheet Data Data from spreadsheets may come in as .txt or .csv files. Ultimately, you will probably want to either (a) compile all of one or two columns into a list (b) or store all the data in something like a pandas DataFrame. • What makes more sense--- .read() or readlines()? • Files containing spreadsheets often have the columns delimited by tabs (tab- delimited .txt files) or commas (.csv). • This, for say, a tab-delimited file, you will want tokenize by splitting on ‘\\t,’ looping through each line: for line in raw_text: tokenized_line = line.split(‘\\t’) Let’s say you just wanted to compile all the data in just the fourth column of the spreadsheet into one list (or list of lists). How could you modify the code above to achieve this? • Now let’s think about getting the data into a DataFrame instead. • The long way--- compile each column as a list: for line in raw_text: tokenized_line = line.split(‘\\t’) names.append(tokenized_line[0]) ages.append(tokenized_line[1]) … data = {‘name’ : names, ‘age’ : ages…} df = pd.DataFrame(data) • Alternatively, pandas has a method for reading in these file types: df = pd.read_csv(“my_csv.csv”) #for csvs df = pd.read_table(“my_table.txt”) #for tables as .txt file or other formats 4. PDFs • PDFs typically require a special program to view its text • You may consider copying the text and saving it in a different format • Or, use the PyPDF2 tool: import PyPDF2 pdffile = open(‘file_name’.pdf', “rb”) #rb means opening in binary format pdfReader = PyPDF2.PdfFileReader(pdffile) #create reader object page = pdfReader.getPage(0) #make page object page_text = page.extractText() #extract text from page • Note that this puts all the text together without spaces; there are other more complicated ways of reading in whitespaces 5. HTML • Websites are typically written in HTML and the text itself is littered with meta tags, JavaScript, tables, etc. • First of all, read in the website as so: import requests url = “url_name.com” page = requests.get(url) Here you might examine what this looks like without some additional processing • How might you deal with cleaning up some of this text if you were coding it yourself? • A commonly used package for cleaning up HTML is BeautifulSoup. Apply the code below to the raw HTML you read in: from bs4 import BeautifulSoup soup = BeautifulSoup(page.content, 'html.parser') clean_text = soup.get_text() 6. Using NLTK tools with your text • Once you have created a tokenized word list from your text, you can analyze that text with NLTK tools like FreqDist and CondFreqDist • Additionally, you can convert your token list to a NLTK Text object: text = nltk.Text(word_list) • Then you can apply the NLTK Text object methods, like .concordance(), .similar(), generate(), etc. 7. Finding Text and Data Where might you find files containing large samples of text that you can use analyze? There are a countless number of sources for text and corpus data today, from literature to movie scripts to social media. A fairly quick search yielded the following results: • Gutenberg Corpus: https://www.gutenberg.org/ - free eBooks in .txt format • The Internet Movie Script Database: https://imsdb.com/ • Data World: https://data.world/ - has collections of tweets, reddit posts, transcripts, etc. • Someone’s compilation of free corpora: https://github.com/niderhoff/nlp- datasets • https://www.corpusdata.org/ --- Lots of good stuff but it seems most of this is not free Exercise: Find a text, read it in, clean it a up and start some basic analysis. This will lead to the first homework assignment.","libVersion":"0.5.0","langs":""}