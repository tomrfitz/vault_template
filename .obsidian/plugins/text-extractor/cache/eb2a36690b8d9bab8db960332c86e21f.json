{"path":"Files/Ling 430 - Handout #12 - Hidden Markov Models.pdf","text":"Ling 430 March 2 nd, 2022 POS Tagging with HMMs 1. Hidden Markov Model • The Hidden Markov Model (HMM) is an advanced algorithm for determining a sequence of states considering both the probability of a state occurring given a particular observation, and the probability of a state occurring given the previous state. • We can apply this to part-of-speech tagging: • The observed data are the words. • The states are the part-of-speech tags, which are “hidden” because they have to be deduced from our observations. • Given a particular word, the possible POS tags have different probabilities • Given a particular part-of-speech, some are more likely to follow others • Thus, we calculate the probabilities of the possible POS tag sequences, and determine which one is most likely 2. Toy Data Set • Let’s apply an HMM to a simplified data set to understand how it works • We have a corpus of six sentences with only three possible POS tags--- noun, modal verb, and verb: 1) Mary Jane can see Will. 2) Spot will see Mary. 3) Will Jane spot Mary? 4) Mary will pat Spot. 5) Will will pat Jane. 6) Pat can see Mary. • As you can see, we have words with ambiguous part-of-speech labels, like ‘will’ (noun or modal verb), ‘spot’ (noun or verb), and ‘pat’ (noun or verb). • Unigram or Lookup tagging will always tag words with their most common tags, so something like “will” will probably always get marked as a modal verb. • Bigram tagging will do better (if our Corpus has examples like (“Will” (N), “said” (V)) vs. (“Will” (Mod), “go” (V)), but typically not as well as an HMM. 3. Emission Probabilities • Emission probabilities relate to how likely a word is to have a particular part-of-speech tag; however, we write the probabilities in terms of: given a particular part-of-speech, how likely (based on observations from our corpus) would it be the observed word? • So let’s calculate the emission probabilities for the words in our mini-corpus above. unique words: mary, jane, can, see, will, spot, pat POS tags: noun, modal, verb Noun Modal Verb mary 5/13 0/6 0/6 jane 3/13 0/6 0/6 can 0/13 2/6 0/6 see 0/13 0/6 3/6 will 2/13 4/6 0/6 spot 1/13 0/6 2/6 pat 2/13 0/6 1/6 i.e., we see 13 examples of nouns in the 6 sentences. 5 of those times, the noun is “Mary,” 3 times that noun is “Jane”, it is never “can” or “see”, etc. 4. Transition Probabilities • The transition probabilities are the likelihood of going from one POS tag to another. • We also want to include sentence start and end boundaries, so know how common it is to start or end a sentence with a particular POS N MOD V <END> <START> 5/6 1/6 0/6 0/6 N 1/13 5/13 1/13 6/13 MOD 1/6 0/6 5/6 0/6 V 6/6 0/6 0/6 0/6 i.e., we have six “starts” of sentences, where five begin with a noun and one with a model verb; 13 examples of nouns, one of which is followed by another noun, five by modals, etc. 5. Calculating the Most Likely Sequence of States • Let’s apply our model to a new sentence: “Jane will spot Will.” The above diagram shows all the possible combinations; all POS with emission probabilities of 0 have been excluded, yet we still have 8 possible combinations (3 words have a choice of two possible tags). 1) <START> N N N N <END> (5/6) * (3/13) * (1/13) * (2/13) * (1/13) * (1/13) * (1/13) * (2/13) * (6/13) = 7.35e-8 2) <START> N N N MOD <END> (5/6) * (3/13) * (1/13) * (2/13) * (1/13) * (1/13) * (1/13) * (4/6) * (0/6) = 0 3) <START> N N V N <END> (5/6) * (3/13) * (1/13) * (2/13) * (1/13) * (2/6) * (6/6) * (2/13) * (6/13) = 4.41e-6 4) <START> N N V MOD <END> (5/6) * (3/13) * (1/13) * (2/13) * (1/13) * (2/6) * (0/6) * (4/6) * (0/6) = 0 5) <START> N MOD N N <END> (5/6) * (3/13) * (5/13) * (4/6) * (1/6) * (1/13) * (1/13) * (2/13) * (6/13) = 3.45e-6 6) <START> N MOD N MOD <END> (5/6) * (3/13) * (5/13) * (4/6) * (1/6) * (1/13) * (5/13) * (4/6) * (0/6) = 0 7) <START> N MOD V N <END> (5/6) * (3/13) * (5/13) * (4/6) * (5/6) * (2/6) * (6/6) * (2/13) * (6/13) = 9.72e-4 8) <START> N MOD V MOD <END> (5/6) * (3/13) * (5/13) * (4/6) * (5/6) * (2/6) * (0/6) * (4/6) * (0/6) = 0 6. Viterbi Algorithm • If we have a sentence with 10 words, and we have a pool of 25 POS labels, the HMM would have 2510, or almost 100 trillion combinations to consider • Fortunately, we can remove many possible paths where any of the tags would have 0 emission probability for a given word • Looking at the above example, we could also eliminate paths with 0 transition probabilities, leaving us with only 4 viable candidates (out of initially 34, or 81, with a four-word sentence and three word categories) • But we can do even better, especially when there still might be millions of candidate paths • Consider the subset paths below--- we have two ways getting to SPOT (V). • Up to this point, the probability of N-N-V is 0.00017, whereas the probability of N- MOD-V is 0.041. No matter what the future values are, the N-MOD-V path will be more likely than the N-N-V path. So the Viterbi algorithm allows up to stop considering the N-N-V path at this point, serving to greatly reduce the number of calculations needed. 7. The Code • Import the pomegranate package and make your model object: import pomegranate model = pomegranate.HiddenMarkovModel() • Write each state--- noun, verb, modal--- which include the emission probabilities: emission_noun = pomegranate.DiscreteDistribution({\"mary\": 5/13, \"jane\" : 3/13, \"can\": \\ 0, \"see\" : 0, \"will\": 2/13, \"pat\": 1/13, \"spot\": 2/13}) state_noun = pomegranate.State(emission_noun, name = \"noun\") Now write the verb and modal states following this format • Add the states to the model: model.add_states(state_noun, state_modal, state_verb) • Now add the transition probabilities to the model for all the combinations (<start> to all three, then noun, verb, and modal to all three + <end>): #from start to any category model.add_transition(model.start, state_noun, 5/6) model.add_transition(model.start, state_modal, 1/6) model.add_transition(model.start, state_verb, 0) #from noun to any category model.add_transition(state_noun, state_noun, 1/13) … • “bake” the model before it can be applied: model.bake() • Optionally you might want to rename the start and end states: model.states[-1].name = \"<E>\" model.states[-2].name = \"<S>\" • Apply the model to an example sentence. The sentence can only contain words in the trained vocabulary. Ordinarily, you’d process the sentence but here I’ve typed it up as a list of words so we can skip those steps: words = ['jane', 'will', 'spot', 'will'] viterbi_likelihood, viterbi_path = model.viterbi(words) path = [state[1].name for state in viterbi_path] • The last line here will output a list containing the predicted POS tags, including the start and end tags","libVersion":"0.5.0","langs":""}