{"path":"Files/Ling 430 - Handout #21 - ASR.pdf","text":"Ling 430 April 22nd, 2022 Speech Recognition: Word and Sound Recognition 1. Challenges of ASR • Automatic speech recognition (ASR) is the process of converting speech to text (STT) • The speech signal contains a lot of acoustic variation. What do you think some of the challenges of training a computer identify sounds, words, or sentences from the acoustic signal? Challenges in ASR: - problem of normalizing--- disregarding variation in pitch, volume, word length, vocal tract size, dialect, etc. - background noise, isolating the target speaker - identifying word boundaries - ambiguity of interpreting strings of phones / homophony, e.g. [ɹɛkənaɪspiʧ] - speech errors, repetitions, incomplete sentences, slips of the tongue, reduction, etc. 2. The Acoustic Model • The goal of the Acoustic Model in the STT pipeline is convert the speech signal into a string of phonemes: 1) Convert waveform into spectrogram using Fourier analysis (done last time) 2) The data needs to be simplified. We can take frequency measurements just at particular intervals or slices. 3) Transform the data to better represent human hearing: - Use the Mel scale (or Bark), logarithmic - Divide up the frequencies into bins - Toss out frequencies too high to hear - More sensitivity to lower frequencies 4) Normalization--- try to remove variation due to individual differences in voices - The source is unique to individuals, but we can extract articulatory information from the filter - Cepstral analysis is an algorithm used to try to isolate the filter - Mel Frequency Analysis + Cepstral Analysis yields 12-13 “features” (they are not the same as articulatory features)--- means of greatly simplifying the data 5) Conversion to phonemes: The values for the features are then used to assign phones or phonemes to the audio signal. A labeled corpus is used to train a model to convert from features to phonemes (supervised machine learning) Dynamic Time Warping is an algorithm used to calculate the similarity between signals of differing lengths. Thus, longer or shorter versions of phonemes are labeled the same. Note, however, that training with the right neural network might handle this for us. 3. The Language Model • ASR incorporates aspects of top-down processing--- because of the variability of speech, some reliance on context is used to clear up ambiguity and assign lexical items to the strings of phonemes • This addresses: - assigning word boundaries - homophony - reduction and deletion of sounds - errors, slips of the tongue, etc. • Probability models, like HMMs, may be used to determine the most likely interpretation of sounds -> words, etc. is “Recognize speech” or “Wreck a nice beach” a more likely sequence? 4. Training a simple ASR model with a neural network • Let’s consider a couple ways of training neural networks to recognize speech: 1) Recognizing words: Convert words to MFCC matrices (input data) with one-hot encoded labels indicating the words. Use CNN or RNN w/ LSTM. 2) Identifying sounds in sequences: Convert words (or even nonsense strings) to MFCC matrices, label with one-hot encoded labels indicating the phonemes in the sequence. Use a Seq2seq model to label the phonemes in new data.","libVersion":"0.5.0","langs":""}