{"path":"Files/Ling 430 - Handout #11 - Part of Speech Tagging.pdf","text":"Ling 430 February 25st – 28th, 2022 Part-of-Speech Tagging 1. Towards Information Extraction • Our goal now is to be able to extraction factual information from sentences, perhaps eventually yielding a table of data such as: relation subject object like Jonathan ice cream like Jonathan donuts hate Mary spinach owns Peter a car • From this information we might be able to: - answer questions - ask questions - assess truth values of statements • In order to extract this information from a sentence we’ll need to: 1) Tokenize the sentence into words ✓ 2) Label the part-of-speech of each word 3) Draw a syntactic tree for the sentence 4) Extract the information now that we understand the relationships between the words 2. Part-of-Speech Tagging • After tokenizing our text, we want to automatically assign POS labels for each word before we can analyze its structure • Brainstorm: What are some ways we could do this with our current skills? • Two sets of tags: full and simplified • The “simplified” tags are closer to what we learn as part-of-speech / word class in linguistics: ADJ (adjective) FW (foreign word) PRO (pronoun) VD (past tense verb) ADV (adverb) MOD (modal verb) P (preposition) VG (present participle) CNJ (conjunction) N (noun) TO (‘to’) VN (past participle) DET (determiner) NP (proper noun) UH (interjection) WH (wh determiner) EX (existential) NUM (number) V (verb) • But full or extended tag sets are also commonly used, such in the Penn Treebank corpus: CC (coordinating conjunction) LS (list item marker) PRP$ (possessive pronoun) VBD (past tense verb) CD (cardinal number) MD (modal) RB (adverb) VBG (gerund or pres. participle) DT (determiner) NN (sng. or mass noun) RBR (comparative adverb) VBN (past participle) EX (existential) NNS (plural noun) RBS (superlative adverb) VBP (non-3s pres. verb) FW (foreign word) NNP (proper noun) RP (particle) VBZ (3s pres. verb) IN (preposition or subordinating conj.) NNPS (plural proper noun) SYM (symbol) WDT (wh-determiner) JJ (adjective) PDT (predeterminer) TO (to) WP (wh-pronoun) JJR (comparative adjective) POS (possessive ending) UH (interjection) WP$ (possessive wh- pronoun) JJS (superlative adjective) PRP (personal pronoun) VB (verb base form) WRB (wh-adverb) 3. Pre-tagged corpora • NLTK’s Brown corpus includes tagged words (list of word-tag pairs) tagged_words = nltk.corpus.brown.tagged_words() the argument simplify = True will give only simplified tags • Penn TreeBank: tagged_words = nltk.corpus.treebank.tagged_words() 4. Automatic Tagging: Default Tagger • The goal is to automatically tag words achieving the highest accuracy possible, so we’ll use different methods on a corpus that’s already tagged, to check our work • The default tagger is a crude method of just assigning the most likely tag to all words • Finding the most common tag: from ntlk.corpus import brown tagged_words = brown.tagged_words(categories = ‘news’) tags = [tag for (word, tag) in tagged_words)] now find the most common tag • Using nltk’s Default Tagger tool, we can tag our corpus and assess the accuracy: tagger = ntlk.DefaultTagger(‘NN’) tag_list = tagger.tag(text) • Check the accuracy: tagged_sents = nltk.corpus.brown.tagged_sents(categories = 'news') print(tagger.evaluate(tagged_sents)) #note the evaluator needs the tagged sentences as the input 5. Tagging with Regular Expressions • Recall: Word classes are defined by their ______ and _______ behavior • We can use this to help us--- we can write regular expressions to match suffixes that are highly correlated with certain parts of speech • Using nltk’s RegexpTagger: first write a list of pattern, tag pairs: patterns = [(r‘.*ing$’ , ‘VBG’) , ( etc., etc.) ] Most words ending in -ing are gerunds or participles (can you think how to improve this RE?). What are some other endings that usually determine a word’s class? • Using tagger and assessing its accuracy: tagger = nltk.RegexpTagger(patterns) tag_list = tagger.tag(text) tagger.evaluate(tagged_sents) 6. Lookup Tagger • A lookup tagger tags words with their most common part-of-speech • We’ll need to compile a dictionary where each word has its most common part-of- speech tag as the key: 1) Create a ConditionalFreqDist object that takes the list of tagged words from the Brown news corpus as its input 2) Loop through the set of words--- in this case all the words in the Brown news corpus 3) For each word, assign the tag with the highest count as the key for each word; The CFD’s method: cfd[word].max() will give you the tag with the highest count, and you could write steps 2 and 3 in one line as a dictionary comprehension • Create the tagger and assess: tagger = nltk.UnigramTagger(model=tag_dictionary) tagged_words = tagger.tag(text) tagger.evaluate(brown_tagged_sents) • We can also specify a “backoff” tagger--- if the word doesn’t occur in the dictionary, use the default tagger: tagger = nltk.UnigramTagger(model = tag_dictionary, backoff = \\ nltk.DefaultTagger(‘NN’) • This model can do quite well but is far from perfect. What are a few reasons it is making errors, and what might be done to improve the tagger’s performance? 7. N-Gram Tagging • Let’s add in a bit of the second characteristic of a word class: syntax • An n-gram is a group of words n words long. Bigrams are pairs of words, trigrams are word triplets. • In the case of bigram tagging, a tag is assigned based on the word itself and the tag of the previous word: the wind DET (‘wind’ followed by DET most likely to be NN) to wind TO (‘wind followed by TO most likely to be VB) • Nltk’s n-gram taggers: i) Unigram tagging is the same as a lookup tagger, however the tagger can also train on a specified corpus instead of us supplying it with a dictionary. It will also be best to apply tagging to words in tokenized sentences, since we do not want to consider the tags of words across word boundaries: tagger = nltk.UnigramTagger(tagged_sents) Just as before, the tagger does unfairly well since it is tagging words from a corpus it was trained on. So we can split the corpus into training and testing split_point = 0.9 * len(tagged_sents) training = tagged_sents[:split_point] testing = tagged_sents[split_point:] ii) Now, bigram tagging. Once again, we can train the tagger on a particular corpus: tagger = nltk.BigramTagger(training_sents) tagger.evaluate(testing_sents) Uh oh! Why doesn’t it seem to work very well? Look at some tags for select sentences to discover what happened. 8. Combined Taggers • A solution to the sparse data problem is to backoff to another tagger when one should fail. Together, the bigram, unigram, and default taggers can achieve fairly good accuracy even on new sentences: default_tagger = nltk.DefaultTagger(‘NN’) unigram_tagger = nltk.UnigramTagger(training_sents, backoff = default_tagger) bigram.tagger = nltk.BigramTagger(training_sents, backoff = unigram_tagger) Exercise: After assessing the accuracy of the bigram tagger, add the TrigramTagger and see how much improvement occurred. Additionally, see if you can get any improvement by adding the cutoff parameter, where you can specify that a tagger needs to have more than n instances, e.g., 2, of a pattern in order not to backoff to the next tagger. 9. Other Possibilities • Unknown words can cause issues for any sort of lookup tagger. You might add a RegexTagger to help identify unknown words. • We could also train a bigram or trigram tagger on unknown words, so it takes the parts-of-speech of the proceeding word(s) into consideration as well • A Brill tagger is another tagging method--- do a decent enough job to start with (say using a Unigram tagger), then apply syntactic rules to make corrections: He had a bad run PRO VBD DET JJ VB access rules: DET JJ VB not likely, DET JJ NN is better PRO VBD DET JJ NN 10. Saving your model • If you train a model on a large corpus it may take time and you want to use it again in the future. You can save a tagger using Pickle as shown below: from pickle import dump output = open('bigram_tagger.pkl', 'wb') #wb = write; will save in working directory dump(bigram_tagger, output, -1) output.close() • And now to load it later: from pickle import load input_file = open('bigram_tagger.pkl', 'rb') # rb = read tagger = load(input_file) input_file.close()","libVersion":"0.5.0","langs":""}