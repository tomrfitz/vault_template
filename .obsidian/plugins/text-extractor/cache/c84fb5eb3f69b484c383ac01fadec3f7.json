{"path":"Files/Ling 430 - Handout #8 - TFIDF.pdf","text":"Ling 430 February 9th, 2022 Topic Analysis and TD-IDF Vectors 1. Topic Analysis â€¢ We are moving towards trying to best characterize what a particular document is about â€¢ Consider the following list of words that have the highest term frequency in a particular Wikipedia article (with words in the title and stop words omitted). What do you think this article is about? Why do you think this? What words led you to this assessment? world, country, has, have, America(n), North, million, population, established, first, war, military, largest, territories, border, native, federal 2. Inverse Document Frequency â€¢ For analyzing a documentâ€™s topic, it doesnâ€™t just matter that a word is used a lot. Itâ€™s more important that this word is used more often than would be expected â€¢ Thus, a word occurring 10 times in a 2500 word document but only occurring in 1% of documents is more important for understanding the idiosyncrasies of a particular document than a word occurring 15 times in that same document but occurring in 50% of documents â€¢ Inverse document frequency is the ratio of the number of documents to the number of documents containing a particular word: IDF[â€˜word1â€™] = 5000/50 = 0.01 #word1 occurs in 50 out of 5000 documents IDF[â€˜word2â€™] = 5000/2500 = 0.5 #word2 occurs in 2500 out of 5000 documents â€¢ We often take the log of this value to minimize some of the seemingly profoundly different IDFs we can get for words that donâ€™t seem to really differ to that degree 3. TF-IDF (Term Frequency â€“ Inverse Document Frequency) â€¢ We then multiply the term frequency of a word by its inverse document frequency to get the TF-IDF--- basically a measure of how peculiar it is for a particular word to show up in a document. This is calculated for each word in the vocab. â€¢ Entire equation: t = a particular word d = the words in particular document D = the collection of documents ğ‘¡ğ‘“(ğ‘¡, ğ‘‘) = ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘¡) ğ‘™ğ‘’ğ‘›(ğ‘‘) ğ‘–ğ‘‘ğ‘“(ğ‘¡, ğ·) = ğ‘™ğ‘œğ‘” ğ‘™ğ‘’ğ‘›(ğ·) ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘‘ğ‘œğ‘ğ‘¢ğ‘šğ‘’ğ‘›ğ‘¡ğ‘  ğ‘ğ‘œğ‘›ğ‘¡ğ‘ğ‘–ğ‘›ğ‘–ğ‘›ğ‘” ğ‘¡) ğ‘¡ğ‘“ğ‘–ğ‘‘ğ‘“(ğ‘¡, ğ‘‘, ğ·) = ğ‘¡ğ‘“(ğ‘¡, ğ‘‘) âˆ— ğ‘–ğ‘‘ğ‘“(ğ‘¡, ğ·) A higher tfidf of a given word in a given document means that word better represents the idiosyncrasies of that document. If just count(t) increases, will tfidf be higher or lower? If just len(d) is higher, will tfidf be higher of lower? If just count(documents containing t) is higher, will tfidf be higher or lower? 4. Exercise â€¢ Letâ€™s replace our TF vectors from the previous exercise with TF-IDF vectors. Youâ€™ll follow the same beginning steps (read in documents, tokenize, normalize, calculate TF for each word), and the final steps (calculating the cosine similarity between documents) but you will need to calculate the IDF, then multiply that by the TF for each word to get the resulting TF-IDF for each word: 1) After compiling the lexicon for all words in all documents, create a dictionary for the IDF values for each word. 2) Loop over the lexicon set. Then loop over each document to check if each word is in the document; If so, add 1 to the tally for that word. After calculating the IDF for each word, add it to the dictionary: idf[word1] = xx.xx 3) Then, loop over the documents as you did before for calculating the TF for each word in the document. But then, multiple each TF by the IDF for the particular word, and then add that to vector for the particular document. 5. TF-IDF Tools â€¢ The sklearn library has a tool for calculating TF-IDF vectors for a corpus of documents: from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer(min_df = 1) model = vectorizer.fit_transform(corpus_of_documents)","libVersion":"0.5.0","langs":""}