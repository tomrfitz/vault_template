{"path":"Files/Ling 430 - Handout #18 - RNNs - LSTMs.pdf","text":"Ling 430 April 8th, 2022 RNNs and LSTMs 1. Neural Networks and Word Order • CNNs improved upon our basic neural network by taking into consideration the proximity of words • However, often word relations extend beyond our small kernel sizes: “The ship’s hull, despite years at sea, millions of tons of cargo, and two mid-sea collisions, shone like new.” • We need some form of memory that applies throughout the length of a document • We also need to be able to handle variable length documents, which CNNs could not 2. Averaging Word Embeddings • We’ve been converting words into word embeddings that quantify the semantic properties of words • We could capture the meaning of entire sentences or documents by averaging up these word vectors. Length of document won’t matter. Here is a toy vocab with 3 dimensional word embeddings: cat = [0.6, 0.2, 0.1] fluffy = [0.3, 0.5, 0.6] sees = [0.3, 0.7, 0.2] dog = [0.4, 0.3, 0.2] “fluffy cat” could be represented as ([0.6, 0.2, 0.1] + [0.3, 0.7, 0.2]) / 2 = [0.45, 0.45, 0.15] • Problem is, this is basically a “bag of words” approach where order doesn’t matter, consider: (the) dog sees (the) cat vs. (the) cat sees (the) dog The averaged word embeddings will be the same for both of these sentences 3. Transition Matrix • However, if we multiply each word vector--- in the order the words appeared in the sentence--- by a set of weights, before adding the next word, we’ll yield different results depending on the order of the words. Let’s consider the following vocab and the following random set of weights as the transition matrix: import numpy as np cat = np.array([0.6, 0.2, 0.1]) sees = np.array([0.3, 0.7, 0.2]) dog = np.array([0.4, 0.3, 0.2]) weights = np.array([[0.2, 0.5, 0.1], [0.4, 0.4, 0.2], [0.7, 0.5, 0.3]]) • Now consider “cat sees dog” vs. “dog sees cat”. Take the first word, multiply it by the transition matrix, add the second word, multiply the result by the transition matrix, then add the third word, and multiply that by the transition matrix: a1 =cat.dot(weights) a2 = a1 + sees a3 = a2.dot(weights) a4 = a3 + dog a5 = a4.dot(weights) print (a5) b1 = dog.dot(weights) b2 = b1 + sees b3 = b2.dot(weights) b4 = b3 + cat b5 = b4.dot(weights) print (b5) The resulting matrices are different: [1.1276 1.3703 0.5347] vs. [1.092 1.4212 0.5212] 4. Recurrent Neural Networks (RNNs) • Recurrent Neural Networks (RNNs) have a rudimentary form of memory similar to that described above. • In the recurrent layer, each word vector is input one at a time, multiplied by the weights, and then the result is passed back through the same neuron along with the next word vector. • So we have n recurrences within the layer, where n is the number of words in the document. Because the words are ordered, we can call each recurrence a timestep (t). • As you see below, you still choose how many hidden neurons are in the recurrent layer, and each of these is recurrent. The “input vector” is the 300-dimensional word embedding, so we have weights for each element in the vector x the number of neurons. • Once the output is calculated, the error is determined and backpropagated. The error will have to backpropagate through each time step in the recurrent layer--- but note that the same weights are used in each time step. Thus, the weights are only updated after backpropagating through the entire recurrent layer, so it can be changed by the optimal amount for the entire function 5. Coding an RNN • We’ll be making minimal modifications to convert our CNN to an RNN • Change: epochs = 2, and maxlen = 400 • We’ll still be padding and truncating the data to the max length, though this is not necessarily needed in an RNN--- however it is necessary in this case since we are feeding the recurrent layer output into a dense layer with 50 neurons • Overall structure: 1) Recurrent layer--- 300 neurons (length of word embeddings)--- which will recur 400 times 2) Dense layer with 50 neurons 3) Output layer with 1 neuron (“positive” or “negative”) • “Flatten” here converts our matrix into a 1-D vector. We need a 1-D set of outputs to feed into the output layer (3), but we have 400 outputs--- one for each time step--- for the 50 different neurons. So Flatten converts 400*50 → 20000 * 1 num_neurons = 50 model = Sequential() model.add(SimpleRNN(num_neurons, return_sequences= True, input_shape = (maxlen, embedding_dims))) model.add(Dropout(.2)) model.add(Flatten()) model.add(Dense(1, activation = 'sigmoid')) model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy']) • How does this work with respect to sentiment analysis? The network outputs a sentiment score (between 0 and 1) at each time step, but that output is ignored; the hidden layer output is fed back into the network along with the next word 6. Assessing the Results • Uh oh! The accuracy of the RNN is lower than for the CNN. • Additionally, it took forever to train. Note that we have 300 input weights * 50 = 15,000 at the first time step. Remember that after the first time step, the hidden layer output and the next word is fed in, so we have 50 more weights for old outputs, plus 1 bias weight. So 351 * 50 = 17,550. Then 20,000 weights from the hidden layer to the output. • Then, we have to backpropagate error over each layer, including the 400 time steps in the recurrent layer. • And why doesn’t it work well? 1) Vanishing gradient problem--- as we backpropagate over many layers (like 400 time steps) the gradient by which we update the weights becomes so small the network can’t really learn 2) Also resulting from the high number of time steps, the features (such as ‘negative’ or ‘positive’ sentiment) of the resulting “thought” vector (comprised of the 400 individual word vectors) may be “averaged” out and less prominent. • Thus, “vanilla” RNNs have limited usefulness with documents of this length • Fortunately, there are solutions to address some the problems with regular RNNs 7. Long short-term memory (LSTM) networks • Long short-term memory networks are an upgrade of basic RNNs that fixes most of its problems • The most prominent difference is the addition of a memory cell that “forgets” unimportant information and stores important information • Just like any RNN, they are “recurrent” so they feed in each word at each time step along with the previous output • Thus, at each time step we feed in the new input, the output from the previous time step, and the memory vector • The memory cell contains four parts: 1) The forget gate 2) The candidate gate a) Candidate choice--- what to remember b) Candidate values--- what values to give the remembered elements 3) The output gate • Each of these parts of the memory cell has its own neural network to determine what it needs to forget or remember • The memory vector will have n neurons--- same number as in your recurrent layer. Those are then updated by the forget, candidate, and output gates: 1) Forget gate: Certain things might not being important any--- e.g., the word ‘not’ too far back is no longer relevant to a verb or adjective. So the forget gate trains, and then output n neurons with values between 0 and 1. This is a “mask” that is applied to the memory vector--- multiplying by 0 removes the information from memory, multiplying by 1 lets it all through: 2) Candidate gate: The “candidate choices” are selected with a gate much like the forget gate (values between 0 and 1). These values are multiplied by the second part--- the candidate values, between -1 and 1. This vector is then added to the memory vector, increasing values of certain elements. 3) Output gate: This applies the memory vector to the actual input. We multiply the memory vector by the hidden state vector, to output the next hidden state. 8. LSTM code Switching from an RNN to LSTM is simple--- import LSTM and change SimpleRNN to LSTM: from tensorflow.keras.layers import LSTM model.add(LSTM(num_neurons, return_sequences= True, input_shape = (maxlen, embedding_dims)))","libVersion":"0.5.0","langs":""}