{"path":"Files/Ling 430 - Handout #17 - CNNs.pdf","text":"Ling 430 April 4th – 6th, 2022 CNNs and using Word2Vec in ANNs 1. Beyond Basic ANNs • From what we’ve learned so far, how do you think basic or “vanilla” artificial neural networks would treat the sentiment of the word “not” together with the underlined words? (or can it consider these “together” at all?) “I did not hate this movie.” “This movie was not bad.” “This was not the first sci fi movie I’ve reviewed, so I can say it was terrible.” • Takeaway: While basic neural networks can find patterns among interacting variables (such as BoW counts), they don’t take into the order and relative distances between words. • Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) are two approaches that will consider syntactic properties of words to some degree 2. Convolutional Neural Networks • CNNs are commonly used in image recognition, and it is perhaps easier to understand the filtering and convolution process with respect to images first • Images consist of pixels with values for brightness. We can convert an image into a matrix of values, and have a neural network identify whether it shows a “cat” (1) or “not a cat” (0) using a neural network • Now think about this picture of a cat. Are there particular pixels whose values might tell us this is cat? • If we want our algorithm to pick up on localized patterns--- things like lines, curves, shapes, etc.--- we need to consider more than pixels and arbitrary combinations of pixels • In a CNN, we first transform the data by applying a filter, or kernel, to small regions of a set size, and then “convolve” over the entire image • The kernel will apply the same weights throughout the data, though updates them with each training iteration. • Above, we see a 5x5 matrix with a 3x3 kernel being applied: 1) The first 3x3 submatrix (0, 25, 75…) is multiplied by the weights (-1, 0, 1…) 2) Those values (0, 0, 75…) are then summed (235) 3) So within the value “235” we have information contained about the first 3x3 block 4) We then apply the same weights to the next 3x3 block (25, 75, 80…), and so on • Notice that the resulting filtered image will be smaller than the original--- only 3x3 since only nine 3x3 sliding matrices will fit in the 5x5 one. • This could result in “undersampling” values near the edge, so we can apply padding: Either this undersampling is ok (padding = ‘valid’) or we can add fake data to the edges to get a resulting 5x5 matrix in this case (padding = ‘same’) • Our CNN could contain more than one convolution layer, and additional dense layers • For convolution layers, we usually apply relu activation • We can then calculate the error in the output and backpropagate all the way to the kernel weights 3. CNNs with Language Data • Now apply the idea of convolution to language--- in the following sentence, how might we describe a kernel convolving over the data? (First think about how you turn this sentence in numbers) “This was not the first sci fi movie I’ve reviewed, so I can say it was terrible.” • A string of words is 1-dimensional rather than 2-d like a flat image. So we can convolve over the sentence in groups of perhaps 3-words: • The data will need to remain sequential, so we won’t want to use Bag-of-Words type models with word counts or Tfidf values. • One-hot vectors could work, but a better solution is to use our lower-dimensional word2vec word embeddings to represent each word: • The dimensions here would be 300 (number of word vectors) by 3 (desired length of the kernel) 4. CNN Code • We’re going to convert our previous sentiment analysis program trained on a basic neural network to a CNN. We can read the reviews and labels in the same: from nltk.corpus import movie_reviews import random pos_documents = [(' '.join(list(movie_reviews.words(doc))), \"pos\") for doc in movie_reviews.fileids(\"pos\")] neg_documents = [(' '.join(list(movie_reviews.words(doc))), \"neg\") for doc in movie_reviews.fileids(\"neg\")] mixed_documents = pos_documents+neg_documents random.shuffle(mixed_documents) value = {'pos': 1, 'neg' : 0} labels = [value[label[1]] for label in mixed_documents] documents = [doc[0] for doc in mixed_documents] • Next, load the Word2Vec model and convert each review into a vector that has words x word vectors: from nltk.tokenize import TreebankWordTokenizer from nlpia.loaders import get_data word_vectors = get_data('word2vec', limit = 200000) def tokenize_and_vectorize(documents): tokenizer = TreebankWordTokenizer() vectorized_data = [] for review in documents: tokens = tokenizer.tokenize(review) sample_vecs = [] for token in tokens: try: sample_vecs.append(word_vectors[token]) except KeyError: pass vectorized_data.append(sample_vecs) return vectorized_data vectorized_data = tokenize_and_vectorize(documents) • Split the data into testing and training: split_point = int(len(vectorized_data) * .8) x_train = vectorized_data[:split_point] y_train = labels[:split_point] x_test = vectorized_data[split_point:] y_test = labels[split_point:] • Next, we need to make the matrix representing each review the same size. If they have more than n words, truncate the review. If they have fewer than n words, pad the matrix with 0s. max_len = 800 #length to cut off reviews at def pad_trunc(data, maxlen): new_data = [] zero_vector = [] for _ in range(len(data[0][0])): zero_vector.append(0.0) for i, sample in enumerate(data): if len(sample) > maxlen: temp = sample[:maxlen] elif len(sample) < maxlen: temp = sample additional_elements = maxlen - len(sample) for _ in range(additional_elements): temp.append(zero_vector) else: temp = sample new_data.append(temp) return new_data x_train = pad_trunc(x_train, maxlen) x_test = pad_trunc(x_test, maxlen) • Now, let’s set up the hyperparameters for our neural network: batch_size = 16 embedding_dims = 300 filters = 250 kernel_size = 2 hidden_dims = 150 epochs = 5 • Convert data from lists to numpy arrays; random seed sets random values so they will not fluctuate each time we run the program: import numpy as np np.random.seed(1337) x_train = np.array(x_train, dtype='float16') y_train = np.array(y_train, dtype = 'float16') x_test = np.array(x_test, dtype='float16') y_test = np.array(y_test, dtype='float16') • Import the objects we need to make the neural network from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout, Activation from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D • Now build the CNN and fit the data: Structure of the model: Input > 1D Convolutional Layer > Dense Layer > Dense Output (1) Two new things: 1) GlobalMaxPooling1D: Helps to reduce the dimensionality of the filters--- while it throws out a lot of information, it keeps the most important things 2) Dropout: This is a technique that helps with overfitting. We want the model to generalize, and not just learn labels for specific reviews. Dropout works by turning off a certain percent of the neurons randomly, which helps the algorithm avoid making overly specific correlations. model = Sequential() model.add(Conv1D(filters, kernel_size, padding = \"valid\", activation = \"relu\", strides = 1, input_shape = (maxlen, embedding_dims))) model.add(GlobalMaxPooling1D()) model.add(Dense(hidden_dims)) model.add(Dropout(0.2)) model.add(Activation('relu')) model.add(Dense(1)) model.add(Activation('sigmoid')) model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy']) model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))","libVersion":"0.5.0","langs":""}