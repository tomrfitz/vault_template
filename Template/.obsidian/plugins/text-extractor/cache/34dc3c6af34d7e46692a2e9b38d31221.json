{"path":"Files/Ling 430 - Handout #2 - Corpora.pdf","text":"Ling 430 January 19 th -21st , 2022 Searching Corpora 1. NLTK Library • The Natural Language Toolkit is a library containing many useful corpora and functions for analyzing text and processing language • If you haven’t installed it: pip install nltk in the command prompt • You may still need to download some texts, but the interpreter will prompt you: nltk.download(xxxxx) • Import the book module: nltk.book import * #this means it will import all the texts for you to access in the interpreter 2. Searching NLTK texts • There are a number of books in the corpus: texts() will list them • NLTK has a number of functions that analyze these texts: syntax function text.concordance(word) gives every instance of the word in the text, with context text.similar(word) words that appear in similar contexts to word text.common_contexts([word1, word2, etc.]) shared contexts of 2+ words text.dispersion_plot([word1, word2, etc,]) plot showing how usage of words changed over time (best with multi-text corpus like the Inaugural Address Corpus text.generate() generates text in the style of the book or corpus text.collocations() word pairs (bigrams) that frequently co- occur Exercises: 1) Find a word or words that are more common in one text than another 2) Find a word or words that are used differently in two different texts 3) Show how the frequencies of some words have changed throughout the Inaugural Address Corpus or another corpus ordered by date 3. Assessing Length and Word Frequency • A couple built-in Python functions: len(text) #gives length--- for these texts will we get characters or words? sorted(text) #sort alphabetically • text.count(word) #NLTK function for counting up individual instances of a word Exercise: Write a new function that quantifies lexical richness. That is, the ratio of words to unique words--- higher lexical richness means more varied word choice. Compare a couple texts. Then compare the word count for couple words, normalized for the length of the text. 4. Frequency Distributions • Could you write a script to tally up the frequency of every word in a text? • Here’s a NLTK shortcut: fdist = FreqDist(text) #create a FreqDist object, which is like a dictionary # text is a list of words fdist.keys() #the list of all distinct words ordered--- let’s view a slice fdist.values() #frequency values fdist.get(word) #gives the frequency of a particular word in the document fdist.N() #total number of samples fdist.max() #sample with the highest count fdist.most_common(n) #gives the n most common words in the text fdist.tabulate() #tabulates the frequency distribution fdist.plot(n, cumulative = True) #plot the n most frequent words, where the curve show the cumulative portion of the words that are accounted for fdist.hapaxes() #words that occur only once in the text 5. String Methods • String objects have a number of built-in methods for assessing and comparing them. These return either True or False, and an be used with conditionals: method syntax description s1.startswith(s2) string s1 begins with s2 (one or more characters) s1.endswith(s2) string s1 ends with s2 (one or more characters) s2 in s1 string s2 occurs somewhere within s1 s1.islower() whether string s1 is all lowercase or not s1.isupper() whether string s1 is all uppercase or not s1.isalpha() whether string s1 is only alphabetic s1.isalnum() whether string s1 is only alphanumeric s1.isdigit() whether string s1 contains only digits s1.istitle() whether string s1 is in title case • The following methods modify a string’s case (which should be assigned to a new variable): method syntax description s1.upper() returns the string in uppercase s1.lower() returns the string in lowercase s1.capitalize() returns the string in title case s1.swapcase() returns the string with all characters in the opposite case 6. Selecting Words with Conditions • How might we go about finding all the words in a text of a particular length (or longer/shorter than some length), a particular frequency (or more/less), or both? • A list comprehension is a short-cut method of producing a list of words that meet certain conditions. Example #1: Looping with if-then conditions long_words = [] #initialize list for word in text: #where text is a list if len(word) > 12: long_words.append(word) Examples #2: Same thing, but with a list comprehension long_words = [word for word in text if len(word) > 12] • We can also use a list comprehension to modify every element in a list: lower_case = [word.lower() for word in text] Exercise: Select a NLTK text, and write a list comprehension collecting words that meet at least two conditions: perhaps length, frequency (from an Fdist object), or containing certain letters, perhaps in certain positions. These can include the string methods discussed earlier. 7. NLTK Corpora • The NLTK library contains additional corpora (sometimes you may be prompted to download these separate from the initial NLTK download) • Usually these contain just a sample of a much larger corpus Name Id contents Gutenberg Corpus gutenberg selection of classic novels Web and Chat Text webtext web chat from discussion forum, overheard conversations, movie scripts, reviews Brown brown 15 different genera, including news, reviews, government, hobbies, fiction, adventure, humor, etc. Reuters reuters news documents Inaugural Address Corpus inaugural 55 presidential inaugural addresses (separate files) Switchboard Corpus switchboard phone conversations CoNLL 2000 Chunking Data conll2000 tagged sentences Penn Treebank treebank tagged and parsed sentences Universal Declaration of Human Rights udhr The UDHR in 300 different languages • Usually these can be accessed with the following code template: import nltk nltk.corpus.corpus_id.method() … where the corpus_id is listed in the second column in the table above, with various methods shown below method description .fileids() OR .fileids([categories]) gives the names of the individual files in the collection, as a list (optionally, only files from certain categories) .categories() for corpora with categories, like Brown, this returns the names of each text category, in a list .raw(categories = [], fileids = []) returns the entire raw contents of the corpus as a string; optionally you can specify to return only documents from a specific category or file .words(categories = [], fileilds = []) returns the corpus as a list of separated words; optionally for just a specific list of categories or files .sents(categories = [], fileids = []) returns the corpus as a list of separated sentences; optionally for just a specific list of categories or files Exercise: Try reading in a couple corpora, exploring its categories and files, and reading in either the entire corpus, specific files or categories, as a raw document, word list or sentence list. 8. Conditional Frequency • Perhaps we want to compare word frequencies in different texts or different categories. How might we program this? • NLTK has a special data type, ConditionalFreqDist, for quickly calculating word frequencies over different conditions: cfd = nltk.ConditionalFreqDist(word_pairs) word_pairs is a list of pairs (a tuple with two elements) of the form (category, word); the function then tallies the number of times each word occurs in each category (such as a genre) 1) We can generate the list of pairs the long way: genres = [“a”, “b”] words = [“c”, “d”, “e”] list_of_pairs = [] for genre in genres: for word in corpus_name.words(categories=genre): if word in words: pair = (genre, word) list_of_pairs.append(pair) cfd = nltk.ConditionalFreqDist(list_of_pairs) 2) OR we can to the same as above, but with a list comprehension: genres = [“a”, “b”] words = [“c”, “d”, “e”] cfd = nltk.ConditionalFreqDist([(genre, word) for genre in genres for word in corpus_name.words(categories=genre) if word in words] • Methods for ConditionalFreqDist(): method syntax description cfd.conditions() returns sorted list of conditions cfd.tabulate() returns a table showing frequencies of the target words by target genre cfd.plot() plots the frequency distributions of the words by genre Exercise: Choose a few words that you expect to have different distributions in different texts or genres. Create a CFD object and tabulate and plot its results. 9. Lexical Corpora • In addition to texts, NLTK includes some corpora that are lists of words and lexical information Name Id contents Words Corpus words all standard English words Stopwords stopwords high frequency and grammatical words. Indicate language in the .words() method; i.g., .words(‘english’) Names Corpus names 8000 first names categorized by gender CMU Pronouncing Dictionary cmudict Each entry is a two-member tuple containing a word and a list of its ARPABET pronunciation. Use method .entries() to view entries Swadesh list swadesh 200-word comparative word lists for about 24 languages. Indicate two-letter language code in .words() method, as in .words(‘en’). Use .entries() to view entries What are some ways you might use these word corpus for a practical or analytical NLP tool? 10. WordNet • Wordnet is a thesaurus-like lexical resource containing information about the semantic connections between words (e.g., synonyms, hypernyms, etc.). • Import the library and retrieve a set of synset identifiers for a word: from nltk.corpus import wordnet print (wordnet.synsets(word)) • This will return in any of the possible meanings of the word in different parts of speech • We can then view a particular synset, e.g.: print (wordnet.synset(‘dog.n.01’).lemma_names() • A lemma is just the base form of a word • Some other useful WordNet methods: method syntax description .lemma_names() returns a list of lemmas for a synset .lemmas() returns the ids of all the synonymous lemmas in a synset .definition() gives the definition of a synset .hyponyms() returns a list of hyponyms of the synset .hypernyms() returns a list of hypernyms of the synset .part_meronyms() returns a list the synset’s part meronyms (components) .substance_meronyms() returns a list the synset’s substance meronyms (things an entity is made of) .member_holonyms() returns the member holonyms, entities which something is a member of .entailments() primarily for verbs, where one action entails something else .antonyms() returns a synset’s antonyms • Some functions can also give a sense of semantic similarity between two synsets: method syntax description synset1.lowest_common_hypernym(synset2) returns the “nearest” shared hypernym between two synsets synset.min_depth() how many layers deep the synset is in the collection of all entities; in other words, it quantifies how specific or general a word is synset1.path_similarity(synset2) calculates the shortest path between two synsets within the net How might we use the path_similarity() method to determine if a word, a, is closer in meaning to word b or word c?","libVersion":"0.5.0","langs":""}