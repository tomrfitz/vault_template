{"path":"Files/Ling 430 - Handout #10 - Topic Modeling.pdf","text":"Ling 430 February 21st – 23rd, 2022 Topic Modeling and LSA 1. Topic Vectors • Our next goal: assess the similarity of documents based on their underlying semantic meaning, not just the words they share • Our TF and TF-IDF vectors contain independent values for every word in the shared vocabulary • I.e., if we have 100 documents that include 5,000 unique words, we describe each document in terms of the frequency of each of 5,000 words • Those 5,000 words thus result in 5,000 dimensional vectors: • We could compare individual people by “features” that are dimensions in the same way (in this case, 3): height (in.) weight (lb.) IQ Henrietta 62 110 135 Sharon 68 150 105 Bubba 70 240 80 • We can then add more and more dimensions, as many as we want. Our documents are something like this: having-the- word-‘the’- ness having-the- word-‘dog’- ness having-the- word- ‘kumquat’-ness +4997 other word dimension… document1 17 0 0 document2 22 5 0 document3 15 0 4 • But ultimately all words are not truly “independent” of each other in meaning--- e.g., documents having the word ‘dog’ may be likely to have the word ‘canine,’ ‘tail,’ ‘bark,’ ‘pet,’ ‘paws,’ ‘beagle,’ etc., since all these words have some underlying semantics in common: the topic of “dog-ness.” • Thus, we can reduce the dimensionality of documents by quantifying their scores on different topics, rather than word counts--- reducing from, say, 5000 to 300 and better capturing the underlying meaning • These are document-topic vectors. We can also characterize words in terms of underlying semantic meaning, called word-topic vectors. Make some of your own word-topic vectors by filling in some appropriate values for each of the words by how well they match each topic: ‘academic’-ness ‘nature’-ness ‘entertainment’- ness ‘architecture’- ness ‘library’ ‘tree’ ‘movie’ ‘book’ • With these vectors, we can calculate the semantic similarity of words (or documents with document-topic vectors) • Note the similarity of ‘library’ and ‘book.’ Unlike word frequency vectors, a search using word-topic vectors might return documents about ‘libraries’ when searching for documents about ‘books.’ This is called a semantic search. 2. Calculating Topics: PCA • We won’t be feeding the computer labeled data--- we want it to find the underlying structure that is there, thus this is an unsupervised machine learning task • How can a machine learn what the topics are? We’ll be using a method called principal component analysis (PCA) with finds clusters of correlating words that define the underlying principle components, or in our case, topics Let’s imagine we have a sets of documents, with the first lines as follows: “Dogs are one of the most popular pets in the United States. Dogs have tails, ears, paws…” “The capital of China is Beijing. The Great Wall of China is one of the most famous…” “I was thinking of getting a new pet. Should I get a dog or a cat? Dogs are great because…” “China is the largest country in the world, with a population over one billion. The history of…” etc. • We can see that we have topics about dogs or pets more generally, and China. • Words that have similar meanings--- and thus are about the same topics, will frequently co-occur. Let’s say we calculate the TF-IDF for each document just looking at two of our many word dimensions: ‘dog(s)’ and ‘paw(s)’: • What we see here is correlation between ‘dog’ and ‘paw.’ They are not completely independent words--- if you have more mentions of ‘dog’ you will more likely have more mention of ‘paw’ • So now let’s convert this from word counts to topic similarity. We have a topic now that seems to involve both ‘dog’ and ‘paw,’ but typically with ‘dog’ maybe occurring 4x as often as ‘paw’ on average • We’ll want to rotate the axes so we have a new “4/5-dog-to-1/5-paw” axis. We do this by finding the line of best fit--- minimizing the distance between each point and the line: • We also add in other axes perpendicular to the new “topic1” or PC1 axis to account for that variation, up to the total numbers of topics there are (which will equal the number of words for now, which is 2 for this example) • PC2 is some “topic” that distinguishes among these articles--- perhaps two articles with similar PC1 scores but different PC2 scores could be about “dogs” vs. “pets,” where the former might include words like ‘paws’ more often. • The PCs are ordered by how much variation in the data they account for. While the algorithm will initially convert n word dimensions into n topic dimensions, we can decide to include only the first x topic dimensions since these may account for a majority of the variation. 3. Under the hood: SVD • PCA is achieved though a mathematical operation known as singular value decomposition (SVD) • The method pulls apart the document-term arrays into three separate arrays that can be multiplied to yield the original array--- like factorization: 12 -> 2 * 2 * 3 • We represent this operation as such: W → U S VT Where W is the term-document array, and U, S, and VT are the three resulting decomposed arrays • Our “W” term-document array: doc1 doc2 doc3 doc4 doc5 doc6 doc7 doc8 doc9 doc10 doc11 dog 15 6 0 0 2 26 17 12 0 1 16 paw 2 1 0 0 0 4 2 1 0 0 2 China 0 1 22 12 0 1 0 0 12 3 0 wall 0 0 2 1 0 0 0 0 34 1 1 bark 1 2 0 0 0 3 2 1 0 0 3 Beijing 0 0 8 18 0 0 0 0 1 1 0 cat 0 4 0 0 24 1 0 11 0 0 0 friend 1 0 0 1 0 3 1 0 1 1 1 largest 1 2 2 1 2 3 0 1 4 5 1 travel 0 0 1 1 0 1 0 0 8 24 0 The svd function in numpy.linalg will decompose the W matrix into the U, S, and VT matrices. np.linalg.svd(term-document array) • The resulting U matrix gives correlations between each word and each topic (range of -1 to 1); higher values means that word more strongly defines that topic. To determine a document’s topic score, multiply each word count by the corresponding word-topic correlation value. 0 1 2 3 4 5 6 7 8 9 dog 0.72 0.62 -0.01 0.21 -0.07 -0.01 0.16 -0.01 -0.09 0.09 paw 0.09 0.08 -0.00 0.05 -0.02 0.00 -0.24 0.28 0.11 -0.91 China 0.35 -0.41 0.58 0.02 -0.06 0.60 0.07 - 0.01 0.07 -0.01 wall 0.44 -0.56 -0.50 -0.05 -0.44 -0.20 0.04 -0.06 -0.04 -0.03 bark 0.09 0.08 0.00 0.03 -0.01 0.01 -0.58 -0.71 0.39 -0.00 Beijing 0.14 -0.17 0.59 0.03 0.04 -0.77 -0.01 -0.03 -0.06 -0.02 cat 0.16 0.15 0.04 -0.97 0.04 -0.01 0.04 -0.01 0.06 -0.03 friend 0.13 -0.04 -0.07 0.04 0.14 -0.10 -0.14 0.53 0.74 0.30 largest 0.16 -0.05 -0.02 -0.05 0.14 0.06 -0.73 0.32 -0.51 0.23 travel 0.25 -0.24 -0.22 0.04 0.87 -0.02 0.17 -0.17 -0.05 -0.11 So notice that for topic #1, the highest correlations are with words like “dog,” “paw,” “bark”, and “cat,” so this has something to do with pets. If we take the dot product of column [1] with each column in matrix W, we find that documents 1, 6, 7, 8, and 10 have the highest scores for topic 1. • The S matrix yields values that quantify how much variation each topic accounts for: [42.7417, 40.921, 26.4964, 25.7936, 23.0028, 9.68264, 2.3596, 1.76015, 0.825625, 0.127156] • The V T matrix yields values quantifying the shared meaning between documents. 4. PCA / LSA • Let’s apply PCA to some real data • PCA applied to linguistic data--- using document-term matrices--- is called Latent Semantic Analysis (LSA) We’re going to apply topic modeling to several years of Elon Musk’s tweets. 1. Find the .csv containing the tweets on Canvas. Read the file in as a dataframe. Some suggestions: i) I removed first two characters from each tweet (b’) which was some sort of Twitter formatting ii) I removed stopwords, as well as some other formatting “words,” etc.: 'xe2', 'x80', 'x9d', 'xa6', 'x99s', 'rt', 'x92', 'x86', 'co', 'http', 'https' 2. Use the TfidfVectorizer to tokenize, remove stop words, and vectorize all in one step 3. Convert the resulting model into a dataframe with the words as columns: df = pd.DataFrame(model.todense(), columns = vectorizer.get_feature_names_out()) 4. Apply LSA: a) Regular PCA; You’ll need to center the data by subtracting the mean first. df = df – df.mean() from sklearn.decomposition import PCA pca = PCA(n_components = 16) dfd = pd.DataFrame(pca.fit_transform(df)) b) “Truncated SVD”; works a bit better with our “sparse” matrix; no centering needed from sklearn.decomposition import TruncatedSVD svd = TruncatedSVD(n_components = 16, n_iter = 100) svd_topic_vectors = svd.fit_transform(df) svd_topic_vectors = pd.DataFrame(svd_topic_vectors) 5. Analyze the resulting topics and the documents that rank highest for those topics: #this will print out the top n documents for each component for item in svd_topic_vectors: top = svd_topic_vectors.nlargest(n, item) for idx in top.index: print (tweets[idx]) print ('\\n') #this prints out the top n words most strongly associated with each component comps = pd.DataFrame(svd.components_, columns = df.columns) for i, row in comps.iterrows(): print (row.nlargest(n)) #shows the explained variance for each component print (svd.explained_variance_) 5. Semantic Search • Create a new tweet that you will vectorize and apply to your SVD model: new_tweet = [\" \"] tokenized_tweet = vectorizer.transform(new_tweet) new_df = pd.DataFrame(tokenized_tweet.todense(), columns = \\ vectorizer.get_feature_names_out()) new_vector = svd.transform(new_df) • Now loop through each of the topic vectors in the training set. Calculate the cosine similarity between it and the new_vector: from scipy import spatial #you can write the loop cos_sim = spatial.distance.cosine(new_vector, svd_topic_vectors.loc[x]) • You can then find the minimal distances, which represent the semantically most similar tweets. How well did it work? 6. Latent Dirichlet Allocation (LDA) • Latent Dirichlet Allocation (LDA) is a different model used for topic modelling. • A Dirichlet distribution is a distribution where the values add up to one, so in this case, the probabilities assigned to each document for each topic total 1 • Pros: Can be more accurate at topic modeling than LSA, with more clearly defined topics and more categorical assignment of texts to just a few topics • Cons: Takes longer to train • How it works: It is generative and probabilistic model, so it works to assign words to topics randomly, and then tries to generate bags of words that match those in the actual documents from sklearn.decomposition import LatentDirichletAllocation as LDA lda = LDA(n_components = 16, learning_method = 'batch') lda = lda.fit(df) components = pd.DataFrame(lda.components_, columns = df.columns)","libVersion":"0.5.0","langs":""}