{"path":"Files/Ling 430 - Handout #13 - Syntactic Parsing.pdf","text":"Ling 430 March 7th – 9th, 2022 Syntactic Parsing 1. Syntactic Parsing • Now that we have applied part-of-speech tagging, we can now parse the syntax of given sentences by grouping words into constituents following our phrase structure rules • We can indicate the phrase structure rules that our grammar will follow as below: grammar = nltk.CFG.fromstring(“““ S → NP VP VP → V NP NP → “John” | “Mary” | Det N Det → “a” | “the” N → “man” | “dog” V → “saw” | “follow” ”””) • Each rule means that the elements on the right form the larger constituent on the left • We can indicate multiple possibilities with the disjunction symbol (|) • Note that for these algorithms, we supply the POS tags for the words as well; though we could imagine a system where these are automatically labeled first with a POS tagger instead 2. Types of Parsers • Discuss: First think how you might instruct the computer to correctly parse a sentence like “John saw the dog” following the above rules: [John [saw [the dog]]] • Most parsers can be classified as utilizing top-down or bottom-up processing. • Top-down: Start with the end goal--- all sentences must eventually reach the S node, which must branch into an NP and VP. A crude version might generate all the possible structures from there, and then check to see which ones would match the particular sentence • Bottom-up: Start with the terminal nodes (the words) and build up from there. Again, a crude version might try every possible way the given words can be combined and see what actually ends up with a single S node. 3. Recursive Descent Parsing • A recursive descent parser is a top-down parser. It basically generates all possible trees given the grammar, and once it generates terminal nodes (words) it sees if they correctly match. It is recursive since for each new node it finds, it then looks inside of those, then inside of those, and then inside of those… • For example, given our mini grammar above, if a RDP tried to parse “Mary saw the dog”: S NP VP (only option given for breaking down S) John VP (first option under “NP”--- but “John” is a terminal not matching “Mary” so we backtrack) Mary VP (match!) Mary (V NP) (the only option given for breaking down VP) Mary (saw NP) (match!) Mary (saw John) (backtrack!) Mary (saw Mary) (backtrack!) Mary (saw (Det N)) Mary (saw (a N)) (backtrack!) Mary (saw (the N)) (match!) Mary (saw (the man)) (backtrack!) Mary (saw (the dog)) (good parse! → NP (V (Det N)) It will then actually still go through all the other possibilities, since multiple structures may match the sentence--- known as structural ambiguity. For example, how might you draw a tree for “I saw an elephant in my pajamas”? • nltk’s RDP tool: from nltk.parse import RecursiveDescentParser sent = ‘your sentence’.split() rd = RecursiveDescentParser(grammar, trace = n) for t in rd.parse(sent): print (t) # • This will print out all the parses that fit. ‘trace,’ if 1-3 will output increasingly verbose descriptions of the algorithm’s steps • Overall, RDP’s aren’t great because they are highly inefficient; they also can’t handle left recursive rules, such as VP → VP Adv (where the left output element is the same as the input), in which case the algorithm gets in an infinite loop (this happens when it backtracks, finds the old VP and expands it to (VP Adv), then has to backtrack again when it doesn’t find any matches, leading to (VP Adv) Adv), etc. 4. Shift-Reduce Parsing • A shift-reduce parser is a bottom up parser. One by one, from left to right, it shifts words to a “stack.” • Whenever the words in the stack match a sequence on the right side of one of the rules, it “reduces” them to new node. • Here is trace output given: [ * the man saw the dog] S [ 'the' * man saw the dog] #words to the left of the * are shifted to the “stack” R [ Det * man saw the dog] #it “reduces” ‘the’ to Det here S [ Det 'man' * saw the dog] #shifts a new word over R [ Det N * saw the dog] #reduces man -> N R [ NP * saw the dog] #now it sees Det N, and can reduce that to NP S [ NP 'saw' * the dog] #shifts ‘saw’ over R [ NP V * the dog] #reduces ‘saw’ to V S [ NP V 'the' * dog] #NP V doesn’t reduce, so ‘the’ is shifted R [ NP V Det * dog] #reduces ‘the’ to Det S [ NP V Det 'dog' * ] #shifts ‘dog’ over R [ NP V Det N * ] #reduces ‘dog’ to N R [ NP V NP * ] #reduces Det N to NP R [ NP VP * ] #reduces V NP to VP R [ S * ] #reduces NP VP to S (S (NP (Det the) (N man)) (VP (V saw) (NP (Det the) (N dog)))) • NLTK’s shift reduce parser doesn’t backtrack, so it may miss some possible parses or find no parse when there is one from nltk.parse import ShiftReduceParser sr = ShiftReduceParser(grammar) for t in sr.parse(sent): print (t) 5. Chart Parsing • Often when backtracking, the algorithm will eventually have to regenerate phrases that were ok • E.g., ‘The cat in the hat…’ might first be parsed as NP PP, only to find that NP cannot be added to PP; so the PP, which was perfectly fine, is erased and the algorithm has to regenerate it (the problem would be in the NP, adding the Det before the PP) • One approach is to use dynamic programming, where the problem is broken down into subtasks (like building the PP, or the NP), and storing successful results for later usage • This is the approach of a chart parser, which stores successful constituents in a well- formed substring table (WFST) • Let’s manually apply chart parsing to find the well-formed substrings in the sentence “The cat in the hat chases a mouse.” 1 2 3 4 5 6 7 8 0 Det 1 N 2 P 3 Det 4 N 5 V 6 Det 7 N • POS tags have been inserted along the diagonal. The indices give the start and end location of each constituent. So ‘Det’ at [0, 1] means this is a constituent beginning at position 0 and ending at 1 (so length = 1). • We can compare increasing larger “spans”--- starting with spans of length 2 (each combination of two adjacent words) up to 8 1 2 3 4 5 6 7 8 0 Det NP 1 N x 2 P 3 Det 4 N 5 V 6 Det 7 N • Above, I’ve begun to fill out the well-formed 2-span substrings. So at [0, 2] we have a well-formed NP comprising Det + N--- beginning at position 0 and ending at 2 (length = two words). At [1, 3] I placed an ‘x’ because N + P does not form a constituent. • Complete the 2-span substrings in the chart above. • We then can work on 3-span substrings. But note here that we have to check all the ways a three-word string can be put together in a single binary joining operation: either 1 element + 2 elements, or 2 elements + 1 element • Thus, for cell [0,3], we have to see if the tags in cells [0,1] and [1,3] can form a constituent AND if cell [0, 2] and [2,3] can form a constituent • Ultimately, if the algorithm finds a full parse, you will get an “S” (sentence) in the top left-most cell • You can then follow the path from the S backwards to find all the trees for the sentence REMOVE: 1 2 3 4 5 6 7 8 0 Det NP x x NP x x S 1 N x x N x x x 2 P x PP x x x 3 Det NP x x x 4 N x x x 5 V x VP 6 Det NP 7 N 6. Additional Considerations • Consider the POS tags for the words in the following sentences: ‘The word ‘the’ has three letters, while ‘was’ has four.’ ‘I can’t adult today.’ • Or how might you parse the following sentence? Are there other possibilities? ‘Time flies like an arrow.’ • Parsers that try to find all possible structures given all possible POS tags can suffer from such pernicious ambiguity • A probabilistic context-free grammar gives likelihoods to each rule, and the overall likelihood of a parse can be calculated: N -> ‘adult’ [0.99] V -> ‘adult’ [0.01] • Of course, context and pragmatic information can lead humans to certain parses over others, a separate and more complicated issue","libVersion":"0.5.0","langs":""}