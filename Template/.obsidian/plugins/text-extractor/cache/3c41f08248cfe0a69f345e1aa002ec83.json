{"path":"Files/Ling 430 - Handout #5 - Tokenization.pdf","text":"Ling 430 January 31 st, 2022 Tokenization 1. Document and Topic Semantics • Our goal: Using computational methods, let’s attempt to characterize the semantic contents of a document. That is to say: - Is a document positive or negative in its overall sentiment? - Can we classify a document, such as an e-mail, as being spam or not? - Can we group documents, say news articles, by topic--- e.g., ‘politics,’ ‘economics,’ ‘science,’ ‘celebrity news,’ etc.? • Discuss: What are some ways that we might “compute” the topic, purpose, or sentiment of a set of documents? Think through the steps you might use to code this. 2. Tokenization: Challenges • The first step in computing a document’s meaning is to split the document into a list of words, which is tokenization. • We’ve already succeeding in crude tokenization, where we split words up based on where spaces occur: word_list = document.split(“ ”) • Split the following sentence based on spaces. Are you satisfied with the outcome? What are the issues that this raises? (Keep in mind the goal--- we want to be able to characterize the document’s meaning) “Thomas Jefferson, the third president, wasn’t alive in 1827.” 3. Splitting with Regular Expressions • One way to deal with punctuation is to write a more complicated split pattern based on regular expressions--- in other words, to include characters / combinations of characters besides “ ” on which to split. • The re.split() method will accomplish this (it splits before and after any matching string): word_list = re.split(regular_expression), text) • Tip: use parentheses ( ) around the regular expression if you want the split element to be returned as well (such as punctuation) • Fill in the regular expression above. Make sure to consider what you want to do about multiple punctuation marks in a row. More advanced: how would you deal with decimal points within numbers? How would you deal with contractions? • Examine your resulting token list from a few examples. Is this satisfactory? Do you have any other ways you might deal with punctuation? Keep the long term goal in mind--- characterizing the meaning of a document. 4. Tokenizers There are a few automatic tokenizers available: a) NLTK’s TreeBank Word Tokenizer --- good all purpose tokenization from nltk.tokenize import TreebankWordTokenizer tokenizer = TreebankWordTokenizer() #create tokenizer object tokens = tokenizer.tokenize(text) #read in text and tokenize b) What about chat and web text, that may be very informal and full of emoticons? text = “RT @TJMonticello Best day everrrrrr at Monticello. Awesommmmmeeeee day :*)” For this, you can use NLTK’s casual_tokenize: from nltk.tokenize.casual import casual_tokenize tokens = casual_tokenize(text) Now try adding the reduce_len = True and/or strip_handles = True arguments.","libVersion":"0.5.0","langs":""}