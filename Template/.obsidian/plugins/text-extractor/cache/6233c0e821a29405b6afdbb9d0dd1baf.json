{"path":"Files/Ling 430 - Handout #14 - Information Extraction.pdf","text":"Ling 430 March 11th, 2022 Information Extraction 1. Named Entity Recognition • In order to start harvesting information from a document and convert into a structured format, we have to identify the entities that we want to collect relational information about • Named entities are specific objects often denoted by proper nouns Types of named entities: ORGANIZATION Rice University, Cincinnati Bengals PERSON President Biden, Noam Chomsky LOCATION Galveston, Brazos River DATE March 7th, 2022 TIME 10:00 am CST MONEY $4.75 USD PERCENT 18.75% FACILITY Herring Hall, Sam Houston Monument GPE (geo-political entity) Houston, Northern Kentucky • We want to be able to automatically label NEs, then extracting information about those NEs. • Discussion: What are some ways you could automatically label all the PERSON NEs in a particular document? Let’s apply our ideas to a Wikipedia article. 2. Sentence Splitting • If we want to apply POS tagging or parse our document, we’ll need to split our text into a list of sentences (each containing a list of words) • At first this might seem easy--- how might you split most sentences? • But now consider the following sentences: Before heading to the U.S., I went to St. John’s. “Help!” he cried. I saw Mrs. Johnson at the store. Consider what REs might help to split some or all of these correctly. • Various packages include sentence tokenizer tools: import spacy tokenizer = spacy.load(‘en_core_web_sm’) nlp = tokenizer(text) for item in nlp.sents: print (item) • You will probably need to download the model, ‘en_core_web_sm’ with the following command in the command module: python -m spacy download en_core_web_sm 3. POS and Dependency Tagging • spaCy will also provide part-of-speech tags (.pos_), more fine-grained POS tags (.tag_), and dependency labels (.dep_, the syntactic function of the word): for item in tokenized: print ((item, item.pos_, item.tag_, item.dep_)) • Exercise: Create a pandas dataframe including the part-of-speech, tag, and dependency of each word in our sample Wikipedia article. Could any of this information help our NE recognition algorithm? 4. Token-based matching with spaCy • Similar to REs, we can write patterns that search for sequences of words based on their POS, dependency, and entity tags, etc. • First load in our nlp model from spaCy and create a Matcher object tokenizer = spacy.load('en_core_web_sm') matcher = Matcher(tokenizer.vocab) • Our word matching pattern is made up of a list of words with certain token (word) attributes. Each word is enclosed with { } braces: pattern = [ { }, { }, { } ] This is pattern template containing three adjacent words that have no token attributes defined. • For each word, some of the following token (word) attributes may be defined: Token attributes description examples ORTH (or TEXT) token match the exact string “ORTH” : “spaCy” (matches a word spelled “spaCy”) LOWER lowercase token matches string “LOWER” : “spacy” (“Spacy” or “spaCy”, etc., will match) LENGTH token must be a certain length “LENGTH” : 5 (any five-letter word will match) POS token must be a certain POS “POS” : “NOUN” (part of speech must be noun) TAG token must be a certain fine- grained POS “TAG” : “NN” (tag must be singular or mass noun) DEP token must have a certain dependency relation “DEP” : “nsubj” (tag must be a nominal subject) LEMMA lemma of token matches string “LEMMA”: “be” (matches ‘is,’ ‘was’, ‘are,’ etc.) ENT_TYPE token must be a certain type of named entity “ENT_TYPE” : “PERSON” (token must be a PERSON entity) OP operation; how often to match the word “OP” : “?” (match 0 or 1 times, i.e., optional. Others include * (0 or more), + (1 or more), ! (none) • For all POS, TAG, and DEP labels see: https://github.com/explosion/spaCy/blob/master/spacy/glossary.py • Or see the documentation for the en_core_web_sm model here (includes NER labels): https://spacy.io/models/en • Some example patterns: pattern = [{\"ENT_TYPE\" : \"PERSON\"},{\"LEMMA\" : \"be\"}] #first word is a “person” entity followed by a form of “be” # e.g.: “Thomas Jefferson was the third president.” pattern = [{\"DEP\" : \"nsubj\"},{\"OP\" : \"?\"}, {\"LEMMA\" : \"go\"}] #first word is a nominal subject, then one option word, and finally a form of “go” #e.g.: “Noam Chomsky last went to England in 2018.” pattern = [{“DEP” : “dobj”, “ENT_TYPE” : “LOC”}, {“POS” : “ADP”}, {“POS” : “DET”, “OP” : “?”}, {“POS” : “ADJ”, “OP” : “*”}, {“POS” : “NOUN”}] #first word is a location entity functioning as a direct object; second word is an adposition, then we have an optional determiner, any number of adjectives, and then finally a noun #e.g.: “Mary saw the mountain by the wide river.” • Now add the pattern to the match object and match using the pattern: matcher.add(“name_of_pattern”, [pattern]) matches = matcher(text) #the object above contains an id, start and stop points within the text for each match that #is found for match_id, start, end in matches: span = tokenized[start:end] print (str(span)) 5. Dependency Matching with spaCy • spaCy allows us to write complex text-matching patterns based on grammatical dependencies between words • Unlike the Matcher, the words in the Dependency Matcher pattern are defined by their hierarchical relations, and not by strings of adjacent words • Here will extract names of authors and books they wrote from a text. 1) Read in the modules and load the language model import spacy from spacy.matcher import DependencyMatcher nlp = spacy.load(\"en_core_web_sm\") 2) Create the Dependency Matcher object: matcher = DependencyMatcher(nlp.vocab) 3) Begin writing the pattern by defining the anchor token. This is the base token to which all the other tokens are related through some dependency. Usually it is best to make the anchor token the head, with other tokens in the pattern being dependents of that head. For this task we want to extract a verb, specifically “write,” and its dependent subject NP and object NP. So we’ll define the verb the anchor token. In most cases, we join tokens, a “left” one (defined earlier) and a new “right” one. But the anchor token begins the pattern, so it is referred to as “right” and it will be the only element in the first token. “RIGHT_ID” is the name given to this token or node, while “RIGHT_ATTRS” defines characteristics of this token. We want it to match the lemma “write.” pattern = [ { \"RIGHT_ID\": \"anchor_write\", \"RIGHT_ATTRS\" : {\"LEMMA\" : \"write\"} } ] 4) Now let’s add our two dependencies to the pattern: the subject and then object of the verb. The anchor token, the verb, is shown again. For the two new nodes, we first indicate the previous token it is linked with. • In both cases, we specify “LEFT_ID” as the original anchor token. • We then indicate the relation the new (right) token has to a previous (left) one. “REP_OP” gives this relationship, which again, for both new tokens, is “>” or “the left token is the immediate head of the right token. • We then name this new token, which we’ll call “write_subj,” for the subject, and “write_obj” for the object. • Finally, we can give characteristics of the new token. We want to indicate that the subject token should have a dependency tag (“DEP”) of “nsubj,” i.e., it is a nominal subject, whereas the object should have a dependency tag of “dobj,” i.e., direct object. pattern = [ { \"RIGHT_ID\": \"anchor_write\", \"RIGHT_ATTRS\" : {\"LEMMA\" : {\"IN\": [\"write\", \"begin\"]}} }, { \"LEFT_ID\" : \"anchor_write\", \"REL_OP\" : \">\", \"RIGHT_ID\" : \"write_subj\", \"RIGHT_ATTRS\" : {\"DEP\" : \"nsubj\"} }, { \"LEFT_ID\" : \"anchor_write\", \"REL_OP\" : \">\", \"RIGHT_ID\" : \"write_obj\", \"RIGHT_ATTRS\" : {\"DEP\": \"dobj\"} } ] 5) We then add the pattern to the previously defined matcher, and name the pattern (in this case “WRITE”). We apply the spaCy model to our text (called ‘clean_text’ here), and apply the dependency matcher to this spaCy document. matcher.add(\"WRITE\", [pattern]) doc = nlp(clean_text) matches = matcher(doc) 6) We then want to interpret and process the resulting matches. Each match has a match id plus an index location for the word within the spaCy document. • I actually want the entire subject and object NPs, not such the noun. So I use the method .subtree to retrieve all the dependents of the subject and object nouns • The “locs” indices will match the order of the tokens in the pattern. So the verb is at locs[0] in the document, the subject as locs[1], and the object at locs[2] all_matches = [] for tok_id, locs in matches: subj = ' '.join([str(word) for word in doc[locs[1]].subtree]) verb = [str(doc[locs[0]]) obj = ' '.join([str(word) for word in doc[locs[2]].subtree])] word_list = [subj, verb, obj] all_matches.append(word_list) 7) Lastly, we’ll want to convert the list of subject, verb, object lists into a more easily analyzable dataframe: import pandas as pd df = pd.DataFrame(all_matches, columns = [\"subject\", \"verb\", \"object\"]) 6. Simple Question Answering • We can now write natural language questions that the computer can answer by referencing the data in our table • There are numerous ways this could be done, but consider the following steps: 1) Write a question that matches the structure of the information you extracted. This might be: - a wh-subject question: “Who wrote Moby Dick?” - a wh-object question: “What did Herman Melville write?” - a yes-no question: “Did Herman Melville write Moby Dick?” -other wh-questions: “When did Herman Melville write Moby Dick?”, etc. 2) Convert the question to a spaCy doc object: inquiry_doc = nlp(inquiry) 3) You’ll need some way to scan for which of the above question types you have, or if you have a question at all. - you could scan for a specific word, like “what” or “who” - It might be better to scan for wh-words, then identify the dependency relation of that word: Who wrote Moby Dick? → [who]nsubj [write]verb [Moby Dick]dobj if subj.tag_ == “WP”: … elif obj.tag_ == “WP”: … 4) Then, search your dataframe for matches to the dependencies other than that of the wh-word. Basically you are finding the information represented by the wh-word: Information table: subj verb obj Herman Melville write Moby Dick Question: subj verb obj who write Moby Dick 5) Return the answer that matches the particular wh-word. This method requires typing in the subject, verb, and object exactly how it would appear in the table, or a match won’t be found. In addition to our normalization techniques (case folding, lemmatization, etc.) what might be another way to return information that best fits the question, even without a perfect match?","libVersion":"0.5.0","langs":""}