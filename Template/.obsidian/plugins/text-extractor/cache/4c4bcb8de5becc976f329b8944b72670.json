{"path":"Files/Ling 430 - Handout #16 - Word2Vec.pdf","text":"Ling 430 March 30th, 2022 Word2Vec 1. Word Vectors • Word vectors are vectors that characterize the meaning of words over a number of dimensions or semantic features • We’ve encountered these before--- LSA produced word-topic vectors. However, Word2Vec is a recent (2013) method of creating word vectors that differs from LSA topic vectors in a few ways: 1) We only consider nearby words in assessing semantic characteristics 2) The model is trained using neural networks 3) A word is usually represented in a vector of a few hundred features (rather than a dozen or two topics) 4) The meaning captured is more precise, and allows us to do some interesting semantic operations (see below) 2. Analogies and other Semantic Operations • Word2Vec essentially allows us to perform mathematical operations on the meaning of words • For example: “What is it called when a woman is a king?” • We can write this out as an analogy: MAN: KING :: WOMAN : ? • If each word is a vector containing feature values, then we can perform the following semantic operation: king – man + woman = ? • This equation should point us to the word ‘queen.’ • We can also put the meaning of different words together in order to point us to a particular concept: “What Polish-French woman was famous for discoveries in physics? • As the following semantic operation: woman + Polish + French + famous + discovery + physics = • Which, if thoroughly trained, Word2Vec may return ‘Marie Curie’ • So how are Word2vec models trained? • Two main methods: Skip-gram and Continuous Bag-of-Words 3. Skip-Gram • The skip-gram method considers a window of size n • These words are the surrounding (context) words of a particular target word • Each word (bold box) will be an input to the neural network. The algorithm is trained to output one of the context words (light box). Here, the window size is 2, with the two preceding and following words being possible outputs. • The network contains three layers: 1) The input word, as a one-hot encoded vector 2) A hidden layer, which will be the size of the number of word features we end up wanting for the word vectors 3) The possible output words using softmax activation--- softmax is best for multinomial outputs (in this case, values for every word in the vocab) where the value is a probability, and all the probabilities will add up to 1. • This shows the word “wrote” trained to predict “Austen”; we will also train the model on the other contextual neighbors, so “wrote” → “Jane,” “wrote” → “Pride” and “wrote” → “and” (and all other words and their neighbors in the training corpus). • We actually don’t use the trained model to predict words--- we want the weight matrix (# of words x # of hidden layers) from the input to the hidden layer. These weights are in fact the word vectors, as they are weighted for each of the n hidden layers. The hidden layers are the abstract features that define the words. 4. Continuous Bag-of-Words • Continuous Bag-of-Words is another method of training a Word2Vec model • More or less opposite of the skip-gram approach: Input the words in the n-sized context window, training them to predict the target word • Input a multi-hot vector with the context words “hot” --- showing “Jane” + “Austen” + “Pride” + “and” to predict “wrote” below: • Which to use? Skip-gram: Better with smaller corpus and for training rare words Continuous bag-of-words: Faster to train, better accuracy for common words 5. Using Word2Vec • We can used pre-trained models that contain vectors for most English words. • Load in the model using the nlpia package: from nlpia.data.loaders import get_data word_vectors = get_data('word2vec') • Alternatively, you can download the model (GoogleNews-vectors-negative300.bin.gz) and load it with the gensim package. Results are more or less identical to the previous model: from gensim.models.keyedvectors import KeyedVectors word_vectors = KeyedVectors.load_word2vec_format('D:/GoogleNews-vectors- negative300.bin.gz', binary=True) • This is memory intensive; If you have problems running this, lower the word limit by adding the limit = n argument to the Keyed_Vectors load statement (NLPIA suggests 200000) • Now you can explore word relations with the .most_similar method. Arguments in the ‘positive’ parameter will be added together, and the top n resulting words will be returned: print (word_vectors.most_similar(positive=['cooking', 'potatoes'], topn=5)) • We can also “subtract” certain words with the ‘negative’ parameter. Let’s try the ‘king’ – ‘man’ + ‘woman’ example: print (word_vectors.most_similar(positive=['king', 'woman'], negative = ['man'], topn=2)) • You can also have the model return the word that is least like the others in the set with the ‘doesnt_match’ parameter: print(word_vectors.doesnt_match(\"leopard tiger beetle octopus tomato\".split())) • Or, return a value that quantifies the similarity between words with the ‘similarity’ paramenter: print (word_vectors.similarity('princess', 'queen')) Exercise: Try out some of your own analogies or other semantic operations. 6. Training a Word2Vec Model • You can also train your own Word2Vec model using your own corpus • Fortunately, you don’t have to write all the neural network architecture yourself • First, import the relevant packages and read in a text (I’m using the KJ Bible): from nltk.corpus import gutenberg kjv = gutenberg.sents('bible-kjv.txt') from gensim.models.word2vec import Word2Vec • Set the hyperparameters: num_features = 300 #num of features--- man-ness, plant-ness, etc. min_word_count = 3 #minimum number of occurrences in text in order to be used for training num_workers = 2 #number of CPU cores to use in training window_size = 6 #context window size subsampling = 1e-3 #subsampling rate for frequent terms • Now train and save the model: model = Word2Vec(kjv, workers = num_workers, vector_size = num_features, min_count=min_word_count, window = window_size, sample = subsampling) model_name = \"kvj_word2vec_model\" model.save(model_name) • Load the model: mod = Word2Vec.load('D:/kvj_word2vec_model') • Explore using the same methods as for the pre-trained model: print(mod.wv.most_similar(\"king\"))","libVersion":"0.5.0","langs":""}